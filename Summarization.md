# Types of evaluation methods 
## Content Overlap Metrics
* ROUGE : [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013) (Lin, 2004)

## Model-based Metrics
* BertScore : [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675) (Zhang, 2019)
* MoverScore : [MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance](https://arxiv.org/abs/1909.02622) (Zhao, 2019)
* BARTScore : [Bartscore: Evaluating generated text as text generation](https://arxiv.org/abs/2106.11520) (Yuan, 2021)
<br>

# Paper Reading List by Jeewoo
1) Abstractive Text Summarization using Sequence-to-Sequence RNNs and beyond (CONLL, 2016)
2) [Fine-tune BERT for Extractive Summarization](https://github.com/nlpyang/BertSum) (2019)
3) [Text Summarization with Pretrained Encoders](https://github.com/nlpyang/PreSumm) (EMNLP, 2019)
4) Unified Language Model Pre-training for Natural Language Understanding and Generation (NIPS, 2019)
5) BART (ACL, 2020)
6) Teaching Machines to Read and Comprehend (NIPS, 2015) (â€» CNN/DM dataset)
7) Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models (2016)
8) [SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization](https://github.com/yixinL7/SimCLS) (ACL, 2021)
9) [SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization](https://paperswithcode.com/paper/summareranker-a-multi-task-mixture-of-experts-1) (ACL, 2022)
10) [BRIO: Bringing Order to Abstractive Summarization](https://github.com/yixinL7/BRIO) (ACL, 2022)
