# Types of evaluation methods 
## Content Overlap Metrics
* ROUGE : [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013) (Lin, 2004)

## Model-based Metrics
* BertScore : [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675) (Zhang, 2019)
* MoverScore : [MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance](https://arxiv.org/abs/1909.02622) (Zhao, 2019)
* BARTScore : [Bartscore: Evaluating generated text as text generation](https://arxiv.org/abs/2106.11520) (Yuan, 2021)
<br>

# Paper Reading List by Jeewoo
1) Abstractive Text Summarization using Sequence-to-Sequence RNNs and beyond (CONLL, 2016)
2) [Fine-tune BERT for Extractive Summarization](https://github.com/nlpyang/BertSum) (2019)
3) [Text Summarization with Pretrained Encoders](https://github.com/nlpyang/PreSumm) (EMNLP, 2019)
4) BART (ACL, 2020)
5) Teaching Machines to Read and Comprehend (NIPS, 2015) (â€» CNN/DM dataset)
6) Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models (2016)
7) [SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization](https://github.com/yixinL7/SimCLS) (ACL, 2021)
8) [BRIO: Bringing Order to Abstractive Summarization](https://github.com/yixinL7/BRIO) (ACL, 2022)
