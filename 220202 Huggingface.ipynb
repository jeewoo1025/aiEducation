{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (4.15.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (4.62.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\suljeewoo\\anaconda3\\envs\\zeze\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from requests->transformers) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: six in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\suljeewoo\\anaconda3\\envs\\zeze\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Tutorial\n",
    "\n",
    "* 설명 : https://github.com/huggingface/transformers/blob/master/README_ko.md\n",
    "* 튜토리얼 : https://www.ohsuz.dev/22f4e8e7-64a3-4789-9dd2-171913883733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 104kB/s]\n",
      "Downloading: 100%|██████████| 681M/681M [00:59<00:00, 12.1MB/s] \n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 4.84kB/s]\n",
      "Downloading: 100%|██████████| 972k/972k [00:01<00:00, 975kB/s] \n",
      "Downloading: 100%|██████████| 1.87M/1.87M [00:01<00:00, 1.02MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "\n",
    "# 원하는 모델 이름은 사이트에서 검색\n",
    "MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenizer\n",
    "* input_ids : 모델의 입력\n",
    "* token_type_ids : 문장을 구분짓는 id (현재 한 문장밖에 입력으로 주지 않았으므로 다 0)\n",
    "* attention_mask : padding을 구분짓음 (padding은 0으로 표시 - 현재 padding이 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key input_ids:\n",
      "\t value tensor([[   101,   9638, 119064,  25387,  10892,  59906,   9694,  46874,   9294,\n",
      "          25387,  11925,    119,    102]])\n",
      "key token_type_ids:\n",
      "\t value tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "key attention_mask:\n",
      "\t value tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "text = \"이순신은 조선 중기의 무신이다.\"\n",
    "\n",
    "tokenized_input_text = tokenizer(text, return_tensors=\"pt\")     # pt = pytorch tensor 형태로 반환\n",
    "for key,value in tokenized_input_text.items():\n",
    "    print(\"key {}:\\n\\t value {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single segment token (str): ['[CLS]', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '[SEP]']\n",
      "Single segment token (int): [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 102]\n",
      "Single segment type       : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Multi segment token (str): ['[CLS]', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '[SEP]', '그는', '임', '##진', '##왜', '##란', '##을', '승', '##리로', '이', '##끌', '##었다', '[SEP]']\n",
      "Multi segment token (int): [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 102, 17889, 9644, 18623, 119164, 49919, 10622, 9484, 100434, 9638, 118705, 17706, 102]\n",
      "Multi segment type       : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "single_seg_input = tokenizer(\"이순신은 조선 중기의 무신이다\")\n",
    "print(\"Single segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(single_seg_input['input_ids'])))\n",
    "print(\"Single segment token (int): {}\".format(single_seg_input['input_ids']))\n",
    "print(\"Single segment type       : {}\".format(single_seg_input['token_type_ids']))\n",
    "print()\n",
    "\n",
    "multi_seg_input = tokenizer(\"이순신은 조선 중기의 무신이다\", \"그는 임진왜란을 승리로 이끌었다\")\n",
    "print(\"Multi segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(multi_seg_input['input_ids'])))\n",
    "print(\"Multi segment token (int): {}\".format(multi_seg_input['input_ids']))\n",
    "print(\"Multi segment type       : {}\".format(multi_seg_input['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '.']\n",
      "[101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 102]\n",
      "[CLS] 이순신은 조선 중기의 무신이다. [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenize_text = tokenizer.tokenize(text)\n",
    "print(tokenize_text)\n",
    "input_ids = tokenizer.encode(text)\n",
    "print(input_ids)\n",
    "decoded_ids = tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '.']\n",
      "[9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119]\n",
      "이순신은 조선 중기의 무신이다.\n"
     ]
    }
   ],
   "source": [
    "# SEP, CLS 제거\n",
    "tokenize_text = tokenizer.tokenize(text)\n",
    "print(tokenize_text)\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "decoded_ids = tokenizer.decode(input_ids, add_special_tokens=False)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신', '##은', '조선']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,   # 5개의 token만 살리고 뒤는 짤라버리자\n",
    "    truncation=True\n",
    ")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9638, 119064, 25387, 10892, 59906]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이순신은 조선'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad token : [PAD]\n",
      "pad token id : 0\n"
     ]
    }
   ],
   "source": [
    "print('pad token :', tokenizer.pad_token)\n",
    "print('pad token id :', tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이순신은 조선 중기의 무신이다. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '리', '##뿔', '##이', '뜨', '##럽', '##거', '므', '##리', '##커', '##럭', '##이', '[UNK]', '냐', '##왜', '##쇼', '[UNK]', '[UNK]']\n",
      "[100, 9238, 119021, 10739, 9151, 118867, 41521, 9308, 12692, 106826, 118864, 10739, 100, 9002, 119164, 119060, 100, 100]\n",
      "[UNK] 리뿔이 뜨럽거 므리커럭이 [UNK] 냐왜쇼 [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "text = \"깟뻬뜨랑 리뿔이 뜨럽거 므리커럭이 케쇽 냐왜쇼 우뤼갸 쳥쇼섀료다혀뚜여\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "decoded_ids = tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. BERT 모델 테스트\n",
    "\n",
    "* [MASK] token을 예측해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신', '##은', '[MASK]', '중', '##기의', '무', '##신', '##이다', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"이순신은 [MASK] 중기의 무신이다.\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8747127652168274,\n",
       "  'token': 59906,\n",
       "  'token_str': '조선',\n",
       "  'sequence': '이순신은 조선 중기의 무신이다.'},\n",
       " {'score': 0.06436415016651154,\n",
       "  'token': 9751,\n",
       "  'token_str': '청',\n",
       "  'sequence': '이순신은 청 중기의 무신이다.'},\n",
       " {'score': 0.010954867117106915,\n",
       "  'token': 9665,\n",
       "  'token_str': '전',\n",
       "  'sequence': '이순신은 전 중기의 무신이다.'},\n",
       " {'score': 0.004647163674235344,\n",
       "  'token': 22200,\n",
       "  'token_str': '##종',\n",
       "  'sequence': '이순신은종 중기의 무신이다.'},\n",
       " {'score': 0.0036106579937040806,\n",
       "  'token': 12310,\n",
       "  'token_str': '##기',\n",
       "  'sequence': '이순신은기 중기의 무신이다.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# [MASK] token을 채운 결과를 반환\n",
    "nlp_fill = pipeline('fill-mask', model=MODEL_NAME)\n",
    "nlp_fill(\"이순신은 [MASK] 중기의 무신이다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* last hidden state: 입력된 문장이 13개의 토큰으로 구성됨 -> 13개의 토큰에 대한 임베딩 차원이 768\n",
    "* pooler output : CLS 토큰에 대한 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\ttensor([[   101,   9638, 119064,  25387,  10892,  59906,   9694,  46874,   9294,\n",
      "          25387,  11925,    119,    102,  17889,   9644,  18623, 119164,  49919,\n",
      "          11489,   9484,  12692,  48533,    102]])\n",
      "token_type_ids:\n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "attention_mask:\n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Token wise output: torch.Size([1, 23, 768]), Pooled output: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# tokenizer의 결과 형식과 model의 입력 형식을 맞춰놓음\n",
    "tokens_pt = tokenizer(\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란에서 승리함\", return_tensors=\"pt\")\n",
    "for key,value in tokens_pt.items():\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))\n",
    "\n",
    "outputs = model(**tokens_pt)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output\n",
    "\n",
    "print(\"\\nToken wise output: {}, Pooled output: {}\".format(last_hidden_state.shape, pooler_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = tokenizer(\"오늘 하루 어떻게 보냈나요?\", return_tensors=\"pt\")\n",
    "sent2 = tokenizer(\"오늘은 어떤 하루를 보내셨나요?\", return_tensors=\"pt\")\n",
    "sent3 = tokenizer(\"이순신은 조선 중기의 무신이다.\", return_tensors=\"pt\")\n",
    "sent4 = tokenizer(\"깟뻬뜨랑 리뿔이 뜨럽거 므리커럭이 케쇽 냐왜쇼 우뤼갸 쳥쇼섀료다혀뚜여\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**sent1)\n",
    "sent_1_pooler_output = outputs.pooler_output\n",
    "\n",
    "outputs = model(**sent2)\n",
    "sent_2_pooler_output = outputs.pooler_output\n",
    "\n",
    "outputs = model(**sent3)\n",
    "sent_3_pooler_output = outputs.pooler_output\n",
    "\n",
    "outputs = model(**sent4)\n",
    "sent_4_pooler_output = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9757], grad_fn=<DivBackward0>)\n",
      "tensor([0.6075], grad_fn=<DivBackward0>)\n",
      "tensor([0.5997], grad_fn=<DivBackward0>)\n",
      "tensor([0.9258], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "print(cos(sent_1_pooler_output, sent_2_pooler_output))\n",
    "print(cos(sent_2_pooler_output, sent_3_pooler_output))\n",
    "print(cos(sent_3_pooler_output, sent_4_pooler_output))\n",
    "print(cos(sent_1_pooler_output, sent_4_pooler_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e9114844bc4f983c6a72c7c0b47baf1a556882cc04396e3f2802d72560f4a61"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
