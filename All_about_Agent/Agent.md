# Agent

ë³¸ ê¸€ì€ [LLM Agentsì— ê´€í•œ ë¹„ì£¼ì–¼ ê°€ì´ë“œ](https://tulip-phalange-a1e.notion.site/LLM-Agents-1b9c32470be2800fa672e82689018fc4)ì„ ìš”ì•½í•œ ì •ë¦¬ë³¸ì´ë‹¤. ì°¸ê³ ë¡œ ì›ë¬¸ì€ [A Visual Guide to LLM Agents](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-llm-agents)ì´ë‹¤.

## ğŸ¥¸ What is Agent?
environmentë¥¼ ê´€ì°°í•˜ì—¬ Planì„ ìˆ˜ë¦½í•˜ê³  Action ì‹¤í–‰í•˜ëŠ” system
![Fig1](./images/Fig1.png)

### í•µì‹¬ êµ¬ì„±ìš”ì†Œ 3ê°€ì§€: Memory, Tools, Planning
`text ê¸°ë°˜ ê´€ì ì—ì„œ ì‘ì„±ë¨`
#### Memory
- LLM: ì´ì „ history(=state)ë¥¼ ê¸°ì–µí•˜ì§€ ëª»í•¨
- Agent: ì „ë°˜ì ì¸ history(=state)ë¥¼ ì¶”ì í•˜ê³  ê´€ë¦¬í•¨ 

ì•„ë˜ 2ê°€ì§€ caseë¡œ êµ¬ì„±ë¨
- Short-Term Memory: inputì— ì´ì „ ëŒ€í™”ì˜ historyë¥¼ í¬í•¨.
- Long-Term Memory: ì˜¤ëœ ê¸°ê°„ ì €ì¥ëœ knowledge spaceë¥¼ í†µí•´ ê°€ì¥ ê´€ë ¨ë„ê°€ ë†’ì€ informationë¥¼ ì¶”ì¶œí•´ì„œ inputì— í•¨ê»˜ ì£¼ì–´ì§.

#### Tools
- íŠ¹ì • LLMsì´ ì™¸ë¶€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ê±°ë‚˜ ì™¸ë¶€ Appì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡í•¨
- ì‚¬ë¡€: ìµœì‹  ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜, íšŒì˜ ì¼ì •ì„ ì¡ëŠ” ë“± íŠ¹ì • í–‰ë™ì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
- [Toolformer](https://arxiv.org/abs/2302.04761) ê°™ì´ LLMsì—ê²Œ Tools ì‚¬ìš©ì„ ì§€ì‹œí•˜ëŠ” Promptë¥¼ ì œê³µí•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼ Tool ì‚¬ìš© ìì²´ì— íŠ¹í™”í•˜ì—¬ modelì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ì„ í¬í•¨í•¨

#### Planning
: ì–´ë–¤ Actionì„ ì·¨í•  ì§€ ê²°ì •í•˜ê¸° ìœ„í•´, Planì„ ìˆ˜ë¦½í•˜ëŠ” ë‹¨ê³„. ì¦‰, ì£¼ì–´ì§„ taskë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„ë“¤ë¡œ ë¶„í• í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•¨.
(Planningì€ ë‚´ìš©ì´ ë§ê¸° ë•Œë¬¸ì— ë‹¤ìŒ sectionìœ¼ë¡œ ë„˜ì–´ê°.)

<br>

## â›“ï¸ Planning
Planningì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” Reasoningì„ ìˆ˜í–‰í•´ì•¼í•¨.

### Reasoning (a.k.a. Thinking)
LLMsì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ì „, "ìƒê°"í•˜ëŠ” ê²½í–¥ì„ ë³´ì´ëŠ” ëª¨ë¸

1) Fine-tuningí•˜ëŠ” ê²½ìš°
- [QwQ](https://qwenlm.github.io/blog/qwq-32b-preview/)

2) íŠ¹ì •í•œ Prompt Engineeringì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
- Few shot prompting: [Language models are few-shot learners.](https://arxiv.org/abs/2005.14165)
- CoT: [Chain-of-thought prompting elicits reasoning in large language models.](https://arxiv.org/abs/2201.11903)
- Zero-shot prompting + "Let's think step-by-step": [Large language models are zero-shot reasoners.](https://arxiv.org/abs/2205.11916)

### ReAct: Reason + Act
Paper: [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)
![Fig 2](./images/Fig2.png)

LLMsì€ ReAct promptë¥¼ í™œìš©í•˜ì—¬ Thought, Action, Observationì˜ cycleì„ ë°˜ë³µí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘ì„ ìˆ˜í–‰í•¨.
- Thought: í˜„ì¬ stateì— ëŒ€í•œ ì¶”ë¡ 
- Action: ì‹¤í–‰í•´ì•¼ë  Actions
- Observation: Actionì— ëŒ€í•œ ì¶”ë¡  

ë‹¨ì : Feedbackì„ ë°›ëŠ” í”„ë¡œì„¸ìŠ¤(=Success/Failureì— ëŒ€í•œ ë¶„ì„í•˜ëŠ” ê³¼ì •)ê°€ ë¹ ì ¸ìˆìŒ 

### Reflection
Paper: [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)
![Fig 3](./images/Fig3.png)

Agentê°€ ì´ì „ì˜ failureë¡œë¶€í„° ë°°ìš°ë„ë¡ ë•ëŠ” ê¸°ë²•. ì¦‰, Feedbackì„ ë°›ì•„ Reflectioní•˜ëŠ” ë‹¨ê³„ê°€ ì¶”ê°€ë¨. 3ê°€ì§€ LLMìœ¼ë¡œ êµ¬ì„±ë¨
- Actor: state observationsì— ê¸°ë°˜í•´ Actionì„ ì„ íƒí•˜ê³  ì‹¤í–‰
- Evaluator: Actorê°€ ë§Œë“  ê²°ê³¼ë¥¼ Scoringí•¨
- Self-reflection: Actorê°€ ì·¨í•œ Actionê³¼ Evaluatorê°€ ìƒì„±í•œ scoreë¥¼ ëŒì•„ë³´ë©° í‰ê°€í•¨.