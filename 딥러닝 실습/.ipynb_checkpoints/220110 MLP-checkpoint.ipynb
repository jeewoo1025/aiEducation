{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871030bb",
   "metadata": {},
   "source": [
    "# Multi-layer perceptrons\n",
    "\n",
    "* Linear regression\n",
    "* Single Layer Perceptron\n",
    "* Multi-Layer Perceptron to slove XOR\n",
    "\n",
    "\n",
    "## 신경망 모델 학습 프로세스\n",
    "1. 데이터 전처리\n",
    "2. model 디자인\n",
    "     - layer 종류, 개수 및 뉴런 개수 설정\n",
    "     - 각 layer마다의 activation function 설정\n",
    "3. Loss function 설정 \n",
    "    - (어떠한 방식으로 학습해야 하는지 방향 설정)\n",
    "4. Optimizer 설정 \n",
    "    - (loss function의 영향 정도를 설정)\n",
    "5. 학습\n",
    "\n",
    "## Linear Regression 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea221c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suljeewoo\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\suljeewoo\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\suljeewoo\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\suljeewoo\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\suljeewoo\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\suljeewoo\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f00005",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "\n",
    "# -1.0 ~ 1.0 중 random하게 값 구함 (1차원)\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "X = tf.placeholder(tf.float32, name=\"X\")  # 담는 자료형은 float, 이름은 X\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "\n",
    "hypothesis = W*X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d74056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\suljeewoo\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35665bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.361393 [0.6262774] [1.0856467]\n",
      "1 0.20749265 [0.54082644] [1.0180064]\n",
      "2 0.15049224 [0.5621858] [0.9980746]\n",
      "3 0.14278059 [0.57158256] [0.9735853]\n",
      "4 0.13599168 [0.58200467] [0.9502352]\n",
      "5 0.12953188 [0.5920396] [0.9273863]\n",
      "6 0.123379014 [0.6018481] [0.9050932]\n",
      "7 0.11751842 [0.61141926] [0.8833353]\n",
      "8 0.11193618 [0.6207605] [0.86210054]\n",
      "9 0.10661913 [0.62987715] [0.84137625]\n",
      "10 0.10155464 [0.63877463] [0.8211501]\n",
      "11 0.09673074 [0.64745826] [0.80141026]\n",
      "12 0.09213599 [0.65593314] [0.7821449]\n",
      "13 0.087759465 [0.66420424] [0.7633427]\n",
      "14 0.083590806 [0.67227656] [0.74499243]\n",
      "15 0.07962015 [0.6801548] [0.7270833]\n",
      "16 0.07583817 [0.6878437] [0.70960474]\n",
      "17 0.0722358 [0.6953477] [0.6925463]\n",
      "18 0.068804525 [0.70267135] [0.67589796]\n",
      "19 0.06553625 [0.70981884] [0.6596498]\n",
      "20 0.062423248 [0.71679467] [0.6437923]\n",
      "21 0.059458096 [0.7236028] [0.628316]\n",
      "22 0.05663378 [0.73024714] [0.6132117]\n",
      "23 0.053943645 [0.73673177] [0.5984705]\n",
      "24 0.05138127 [0.7430606] [0.58408374]\n",
      "25 0.048940644 [0.7492372] [0.5700427]\n",
      "26 0.04661593 [0.75526536] [0.5563393]\n",
      "27 0.044401634 [0.76114863] [0.5429653]\n",
      "28 0.0422925 [0.76689047] [0.52991277]\n",
      "29 0.040283605 [0.77249426] [0.517174]\n",
      "X: 6, Y: [5.1521397]\n",
      "X: 2.7, Y: [2.6029086]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(30):\n",
    "        # 동시에 같이 학습을 진행\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X:x_data, Y:y_data})\n",
    "        # 값을 출력 and W,b를 다시 랜덤으로 할당함\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "    \n",
    "    print(\"X: 6, Y:\", sess.run(hypothesis, feed_dict={X:6}))\n",
    "    print(\"X: 2.7, Y:\", sess.run(hypothesis, feed_dict={X:2.7}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36209dc9",
   "metadata": {},
   "source": [
    "## Single Layer Perceptron 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb35e23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, cost:0.11831200122833252, W:[[0.05757832]\n",
      " [0.66730666]], b:[-0.03264108]\n",
      "step:1, cost:0.13228842616081238, W:[[0.0497833 ]\n",
      " [0.65458035]], b:[-0.09458251]\n",
      "step:2, cost:0.14615106582641602, W:[[0.0441192]\n",
      " [0.6432525]], b:[-0.15339544]\n",
      "step:3, cost:0.15968351066112518, W:[[0.04045525]\n",
      " [0.6333105 ]], b:[-0.20912708]\n",
      "step:4, cost:0.1727103292942047, W:[[0.03864256]\n",
      " [0.6247173 ]], b:[-0.26187217]\n",
      "step:5, cost:0.1850993037223816, W:[[0.0385244]\n",
      " [0.6174184]], b:[-0.31175858]\n",
      "step:6, cost:0.19675832986831665, W:[[0.03994399]\n",
      " [0.61134756]], b:[-0.358935]\n",
      "step:7, cost:0.20763030648231506, W:[[0.04274984]\n",
      " [0.606432  ]], b:[-0.40356138]\n",
      "step:8, cost:0.21768730878829956, W:[[0.04679918]\n",
      " [0.60259587]], b:[-0.4458016]\n",
      "step:9, cost:0.22692380845546722, W:[[0.05195977]\n",
      " [0.59976315]], b:[-0.48581815]\n",
      "step:10, cost:0.23535141348838806, W:[[0.05811075]\n",
      " [0.5978596 ]], b:[-0.5237688]\n",
      "step:11, cost:0.24299390614032745, W:[[0.06514262]\n",
      " [0.59681386]], b:[-0.55980414]\n",
      "step:12, cost:0.2498834878206253, W:[[0.07295678]\n",
      " [0.59655833]], b:[-0.59406626]\n",
      "step:13, cost:0.25605738162994385, W:[[0.08146485]\n",
      " [0.59702957]], b:[-0.6266882]\n",
      "step:14, cost:0.2615564167499542, W:[[0.09058781]\n",
      " [0.5981684 ]], b:[-0.6577937]\n",
      "step:15, cost:0.2664220333099365, W:[[0.10025509]\n",
      " [0.5999199 ]], b:[-0.6874975]\n",
      "step:16, cost:0.2706959843635559, W:[[0.11040369]\n",
      " [0.60223335]], b:[-0.7159055]\n",
      "step:17, cost:0.2744193971157074, W:[[0.12097735]\n",
      " [0.6050618 ]], b:[-0.74311525]\n",
      "step:18, cost:0.27763208746910095, W:[[0.1319258]\n",
      " [0.6083621]], b:[-0.7692167]\n",
      "step:19, cost:0.28037163615226746, W:[[0.14320405]\n",
      " [0.61209446]], b:[-0.7942924]\n",
      "step:20, cost:0.2826744318008423, W:[[0.15477175]\n",
      " [0.6162222 ]], b:[-0.81841844]\n",
      "step:21, cost:0.2845742404460907, W:[[0.16659266]\n",
      " [0.6207116 ]], b:[-0.84166473]\n",
      "step:22, cost:0.2861032783985138, W:[[0.1786342]\n",
      " [0.6255317]], b:[-0.86409545]\n",
      "step:23, cost:0.2872912883758545, W:[[0.19086698]\n",
      " [0.6306537 ]], b:[-0.8857697]\n",
      "step:24, cost:0.28816622495651245, W:[[0.20326443]\n",
      " [0.63605124]], b:[-0.9067421]\n",
      "step:25, cost:0.2887544333934784, W:[[0.21580255]\n",
      " [0.6417    ]], b:[-0.9270626]\n",
      "step:26, cost:0.2890799343585968, W:[[0.22845954]\n",
      " [0.64757746]], b:[-0.9467774]\n",
      "step:27, cost:0.2891656458377838, W:[[0.24121562]\n",
      " [0.65366274]], b:[-0.96592927]\n",
      "step:28, cost:0.2890323996543884, W:[[0.2540528 ]\n",
      " [0.65993667]], b:[-0.98455733]\n",
      "step:29, cost:0.288699746131897, W:[[0.26695463]\n",
      " [0.6663814 ]], b:[-1.0026978]\n",
      "step:30, cost:0.28818589448928833, W:[[0.2799062]\n",
      " [0.6729806]], b:[-1.0203843]\n",
      "step:31, cost:0.287507563829422, W:[[0.2928939]\n",
      " [0.6797189]], b:[-1.0376477]\n",
      "step:32, cost:0.28668051958084106, W:[[0.30590522]\n",
      " [0.6865822 ]], b:[-1.0545168]\n",
      "step:33, cost:0.2857191860675812, W:[[0.31892878]\n",
      " [0.69355744]], b:[-1.0710177]\n",
      "step:34, cost:0.28463682532310486, W:[[0.33195418]\n",
      " [0.70063245]], b:[-1.087175]\n",
      "step:35, cost:0.2834461033344269, W:[[0.34497193]\n",
      " [0.70779604]], b:[-1.1030115]\n",
      "step:36, cost:0.28215816617012024, W:[[0.35797334]\n",
      " [0.71503776]], b:[-1.1185479]\n",
      "step:37, cost:0.28078392148017883, W:[[0.37095046]\n",
      " [0.7223481 ]], b:[-1.1338036]\n",
      "step:38, cost:0.27933305501937866, W:[[0.38389602]\n",
      " [0.729718  ]], b:[-1.1487967]\n",
      "step:39, cost:0.277814656496048, W:[[0.39680344]\n",
      " [0.7371394 ]], b:[-1.1635436]\n",
      "step:40, cost:0.276236891746521, W:[[0.40966672]\n",
      " [0.7446045 ]], b:[-1.1780595]\n",
      "step:41, cost:0.27460765838623047, W:[[0.42248037]\n",
      " [0.7521064 ]], b:[-1.1923587]\n",
      "step:42, cost:0.2729339301586151, W:[[0.43523943]\n",
      " [0.7596386 ]], b:[-1.2064545]\n",
      "step:43, cost:0.27122220396995544, W:[[0.44793946]\n",
      " [0.76719505]], b:[-1.2203591]\n",
      "step:44, cost:0.269478440284729, W:[[0.4605764]\n",
      " [0.7747702]], b:[-1.2340833]\n",
      "step:45, cost:0.26770809292793274, W:[[0.4731466 ]\n",
      " [0.78235906]], b:[-1.2476379]\n",
      "step:46, cost:0.26591622829437256, W:[[0.48564678]\n",
      " [0.789957  ]], b:[-1.2610323]\n",
      "step:47, cost:0.26410728693008423, W:[[0.49807405]\n",
      " [0.7975597 ]], b:[-1.2742757]\n",
      "step:48, cost:0.2622854709625244, W:[[0.5104258]\n",
      " [0.8051632]], b:[-1.2873758]\n",
      "step:49, cost:0.2604546844959259, W:[[0.52269983]\n",
      " [0.812764  ]], b:[-1.3003404]\n",
      "step:50, cost:0.25861793756484985, W:[[0.53489405]\n",
      " [0.8203588 ]], b:[-1.3131765]\n",
      "step:51, cost:0.25677886605262756, W:[[0.5470067 ]\n",
      " [0.82794464]], b:[-1.3258907]\n",
      "step:52, cost:0.2549399435520172, W:[[0.5590364]\n",
      " [0.8355188]], b:[-1.3384888]\n",
      "step:53, cost:0.25310376286506653, W:[[0.57098174]\n",
      " [0.8430788 ]], b:[-1.3509763]\n",
      "step:54, cost:0.2512727379798889, W:[[0.5828417]\n",
      " [0.8506224]], b:[-1.3633585]\n",
      "step:55, cost:0.24944883584976196, W:[[0.5946154]\n",
      " [0.8581477]], b:[-1.3756399]\n",
      "step:56, cost:0.24763378500938416, W:[[0.60630214]\n",
      " [0.86565274]], b:[-1.3878249]\n",
      "step:57, cost:0.245829239487648, W:[[0.61790144]\n",
      " [0.87313604]], b:[-1.3999175]\n",
      "step:58, cost:0.24403683841228485, W:[[0.62941283]\n",
      " [0.88059604]], b:[-1.4119215]\n",
      "step:59, cost:0.2422575056552887, W:[[0.64083606]\n",
      " [0.88803154]], b:[-1.42384]\n",
      "step:60, cost:0.240492582321167, W:[[0.6521711 ]\n",
      " [0.89544135]], b:[-1.4356765]\n",
      "step:61, cost:0.23874296247959137, W:[[0.6634178]\n",
      " [0.9028245]], b:[-1.4474337]\n",
      "step:62, cost:0.23700948059558868, W:[[0.6745764 ]\n",
      " [0.91018015]], b:[-1.4591143]\n",
      "step:63, cost:0.23529283702373505, W:[[0.68564695]\n",
      " [0.9175074 ]], b:[-1.4707208]\n",
      "step:64, cost:0.23359373211860657, W:[[0.6966299]\n",
      " [0.9248057]], b:[-1.4822555]\n",
      "step:65, cost:0.23191241919994354, W:[[0.70752543]\n",
      " [0.9320745 ]], b:[-1.4937203]\n",
      "step:66, cost:0.23024967312812805, W:[[0.71833414]\n",
      " [0.93931323]], b:[-1.5051175]\n",
      "step:67, cost:0.22860559821128845, W:[[0.7290564]\n",
      " [0.9465216]], b:[-1.5164489]\n",
      "step:68, cost:0.22698049247264862, W:[[0.7396928]\n",
      " [0.9536991]], b:[-1.5277159]\n",
      "step:69, cost:0.22537463903427124, W:[[0.75024396]\n",
      " [0.9608457 ]], b:[-1.5389204]\n",
      "step:70, cost:0.2237880825996399, W:[[0.7607106 ]\n",
      " [0.96796113]], b:[-1.5500636]\n",
      "step:71, cost:0.22222089767456055, W:[[0.7710933]\n",
      " [0.9750452]], b:[-1.561147]\n",
      "step:72, cost:0.22067323327064514, W:[[0.7813929]\n",
      " [0.9820979]], b:[-1.5721717]\n",
      "step:73, cost:0.2191450148820877, W:[[0.79161006]\n",
      " [0.9891192 ]], b:[-1.5831391]\n",
      "step:74, cost:0.21763619780540466, W:[[0.80174565]\n",
      " [0.99610895]], b:[-1.59405]\n",
      "step:75, cost:0.21614666283130646, W:[[0.8118004]\n",
      " [1.0030674]], b:[-1.6049057]\n",
      "step:76, cost:0.21467632055282593, W:[[0.82177526]\n",
      " [1.0099944 ]], b:[-1.615707]\n",
      "step:77, cost:0.2132250964641571, W:[[0.83167106]\n",
      " [1.0168902 ]], b:[-1.6264548]\n",
      "step:78, cost:0.21179288625717163, W:[[0.8414886]\n",
      " [1.0237547]], b:[-1.6371502]\n",
      "step:79, cost:0.21037928760051727, W:[[0.85122883]\n",
      " [1.0305884 ]], b:[-1.6477935]\n",
      "step:80, cost:0.20898431539535522, W:[[0.8608926]\n",
      " [1.0373912]], b:[-1.658386]\n",
      "step:81, cost:0.2076077163219452, W:[[0.87048084]\n",
      " [1.0441635 ]], b:[-1.6689279]\n",
      "step:82, cost:0.20624910295009613, W:[[0.87999445]\n",
      " [1.0509053 ]], b:[-1.6794201]\n",
      "step:83, cost:0.20490838587284088, W:[[0.8894343]\n",
      " [1.057617 ]], b:[-1.6898633]\n",
      "step:84, cost:0.20358537137508392, W:[[0.89880127]\n",
      " [1.0642985 ]], b:[-1.7002581]\n",
      "step:85, cost:0.20227979123592377, W:[[0.9080964]\n",
      " [1.0709504]], b:[-1.710605]\n",
      "step:86, cost:0.20099127292633057, W:[[0.91732043]\n",
      " [1.0775727 ]], b:[-1.7209047]\n",
      "step:87, cost:0.19971966743469238, W:[[0.9264744]\n",
      " [1.0841658]], b:[-1.7311575]\n",
      "step:88, cost:0.19846463203430176, W:[[0.93555915]\n",
      " [1.09073   ]], b:[-1.7413641]\n",
      "step:89, cost:0.197225883603096, W:[[0.9445756]\n",
      " [1.0972654]], b:[-1.7515249]\n",
      "step:90, cost:0.19600331783294678, W:[[0.9535246]\n",
      " [1.1037724]], b:[-1.7616403]\n",
      "step:91, cost:0.19479648768901825, W:[[0.96240705]\n",
      " [1.1102512 ]], b:[-1.7717109]\n",
      "step:92, cost:0.19360513985157013, W:[[0.9712239]\n",
      " [1.1167021]], b:[-1.781737]\n",
      "step:93, cost:0.1924290955066681, W:[[0.9799759]\n",
      " [1.1231253]], b:[-1.7917191]\n",
      "step:94, cost:0.19126807153224945, W:[[0.9886639]\n",
      " [1.1295213]], b:[-1.8016576]\n",
      "step:95, cost:0.19012172520160675, W:[[0.9972889]\n",
      " [1.13589  ]], b:[-1.8115529]\n",
      "step:96, cost:0.18898986279964447, W:[[1.0058516]\n",
      " [1.1422321]], b:[-1.8214054]\n",
      "step:97, cost:0.18787217140197754, W:[[1.0143529]\n",
      " [1.1485474]], b:[-1.8312157]\n",
      "step:98, cost:0.1867685168981552, W:[[1.0227935]\n",
      " [1.1548367]], b:[-1.8409839]\n",
      "step:99, cost:0.18567848205566406, W:[[1.0311744]\n",
      " [1.1610998]], b:[-1.8507103]\n",
      "[[0.13578951]\n",
      " [0.33411974]\n",
      " [0.3058622 ]\n",
      " [0.58457035]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[0,0],[0,1],[1,0], [1,1]]\n",
    "y_data = [[0], [0], [0], [1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([2,1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "logits = tf.add(tf.matmul(X,W), b)\n",
    "output = tf.nn.sigmoid(logits)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(output - Y))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = opt.minimize(cost)\n",
    "\n",
    "# 학습 시작!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        for x,y in zip(x_data, y_data):\n",
    "            _, cost_val = sess.run([train_op, cost], feed_dict={X:[x], Y:[y]})\n",
    "        print('step:{}, cost:{}, W:{}, b:{}'.format(step, cost_val, sess.run(W), sess.run(b)))\n",
    "        \n",
    "    print(sess.run(output, feed_dict={X:x_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed3369b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, cost:0.6953345537185669\n",
      "step:5000, cost:0.6777076721191406\n",
      "step:10000, cost:0.2077089250087738\n",
      "step:15000, cost:0.03260461241006851\n",
      "[[0.01104391]\n",
      " [0.9844152 ]\n",
      " [0.9844015 ]\n",
      " [0.02060699]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[0,0],[0,1],[1,0], [1,1]]\n",
    "y_data = [[0], [1], [1], [0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# First Layer\n",
    "W1 = tf.Variable(tf.random_uniform([2,3], -1.0, 1.0))\n",
    "b1 = tf.Variable(tf.random_uniform([3], -1.0, 1.0))\n",
    "\n",
    "logits = tf.add(tf.matmul(X,W1), b1)\n",
    "hidden = tf.nn.sigmoid(logits)\n",
    "\n",
    "# Second Layer\n",
    "W2 = tf.Variable(tf.random_uniform([3,1], -1.0, 1.0))\n",
    "b2 = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "logits2 = tf.add(tf.matmul(hidden, W2), b2)\n",
    "output = tf.nn.sigmoid(logits2)\n",
    "\n",
    "# 손실함수 문제였음....!! cross entropy 함수\n",
    "cost = tf.reduce_mean(-1*((Y*tf.log(output))+((1-Y)*tf.log(1.0-output))))\n",
    "\n",
    "# optimizer function\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "train_op = opt.minimize(cost)\n",
    "\n",
    "# 학습 시작!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(20000):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step%5000==0:    \n",
    "            print('step:{}, cost:{}'.format(step, cost_val))\n",
    "        \n",
    "    print(sess.run(output, feed_dict={X:x_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c02a2",
   "metadata": {},
   "source": [
    "## 모델학습 관련 개념\n",
    "* Epoch : 전체 Sample 데이터를 학습하는 것\n",
    "* Step : 1 step 당 weight와 bias를 1회씩 업데이트하게 됨\n",
    "* Batch size : 1 step에서 사용한 데이터의 수\n",
    "* Learning rate : 경사하강법에서 학습 단계별로 움직이는 학습 속도\n",
    "\n",
    "### 조교님 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f074ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7053051\n",
      "1000 0.02538296\n",
      "2000 0.010385898\n",
      "3000 0.0064924695\n",
      "4000 0.0047153435\n",
      "5000 0.0036998042\n",
      "6000 0.0030432646\n",
      "7000 0.002584171\n",
      "[[2.9119849e-04]\n",
      " [9.9738729e-01]\n",
      " [9.9737298e-01]\n",
      " [3.4368038e-03]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[0,0],[0,1],[1,0], [1,1]]\n",
    "y_data = [[0], [1], [1], [0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2,3], -1., 1.))\n",
    "b1 = tf.Variable(tf.random_uniform([3], -1., 1.))\n",
    "L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_uniform([3,1], -1., 1.))\n",
    "b2 = tf.Variable(tf.random_uniform([1], -1., 1.))\n",
    "output = tf.sigmoid(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(output) + (1-Y)*tf.log(1-output))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=1)\n",
    "train_op = opt.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(8000):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step%1000==0:\n",
    "            print(step, cost_val)\n",
    "        \n",
    "    print(sess.run(output, feed_dict={X:x_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d90835",
   "metadata": {},
   "source": [
    "output이 2개 이상되면 <b>multinomial output</b>이라고 한다. 이러한 output에 대해서는 softmax라는 활성화함수를 사용한다!\n",
    "\n",
    "**Why?** <br>\n",
    "차이를 정규화시켜 모든 값들의 합이 1이 되게하는 활성화함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f1a8fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.70499027\n",
      "1000 0.007767747\n",
      "2000 0.0024548844\n",
      "3000 0.0014344295\n",
      "4000 0.0010081355\n",
      "5000 0.0007752951\n",
      "6000 0.00062895234\n",
      "7000 0.000528638\n",
      "predict : [[ 3.4093776 -4.1623507]\n",
      " [-3.4584816  4.029432 ]\n",
      " [-3.7484083  4.224194 ]\n",
      " [ 3.5216434 -4.293834 ]]\n",
      "predict with softmax : [[9.9948549e-01 5.1453715e-04]\n",
      " [5.5949675e-04 9.9944049e-01]\n",
      " [3.4466173e-04 9.9965537e-01]\n",
      " [9.9959677e-01 4.0327950e-04]]\n",
      "predict with argmax : [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[0,0],[0,1],[1,0], [1,1]]\n",
    "y_data = [[1,0], [0,1], [0,1], [1,0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2,3], -1., 1.))\n",
    "b1 = tf.Variable(tf.random_uniform([3], -1., 1.))\n",
    "L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_uniform([3,2], -1., 1.))\n",
    "b2 = tf.Variable(tf.random_uniform([2], -1., 1.))\n",
    "logits = tf.matmul(L1, W2) + b2\n",
    "output_softmax = tf.nn.softmax(logits)\n",
    "output_argmax = tf.argmax(logits, 1)\n",
    "\n",
    "# logits에 들어가는 parameter는 WX + b이여야 한다.\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=1)\n",
    "train_op = opt.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(8000):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step%1000==0:\n",
    "            print(step, cost_val)\n",
    "        \n",
    "    print(\"predict :\", sess.run(logits, feed_dict={X:x_data}))\n",
    "    print(\"predict with softmax :\", sess.run(output_softmax, feed_dict={X:x_data}))\n",
    "    print(\"predict with argmax :\", sess.run(output_argmax, feed_dict={X:x_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c5c18c",
   "metadata": {},
   "source": [
    "최종 출력에 softmax를 취하면, 분포가 더 두드러지는 걸 확인할 수 있다!\n",
    "\n",
    "## MNIST data\n",
    "* 이미지와 이미지에 해당하는 라벨로 구성함\n",
    "* 28x28 pixel : 784차원의 vector로 표현\n",
    "    * 1 pixel : 0(흰색), 1(검은색)\n",
    "    * linear하게 1x784로 존재하게 됨!\n",
    "* label은 one-hot encoding으로 표현됨 (해당 label의 index만 1, 나머지는 0인 벡터)\n",
    "\n",
    "image : 28x28 pixel → 784 vector <br>\n",
    "label : 0~9 10개 숫자 → 1x10 shape의 vector\n",
    "<br><br>\n",
    "\n",
    "<strong>구조</strong>\n",
    "* 총 70,000개의 data로 구성됨\n",
    "    * <strong>training data</strong> : 55,000개, 학습을 하는데 필요한 data\n",
    "    * <strong>test data</strong> : 10,000개, 성능을 평가하는데 사용하는 data\n",
    "    * <strong>validation data</strong> : 5,000개, 모델을 학습 도중에 overfitting을 확인하는데 사용하는 data\n",
    "    \n",
    "<br>\n",
    "- hidden layer 갯수 늘리면 정확도 낮아짐\n",
    "- hidden layer의 node 갯수 늘리면 정확도 낮아짐\n",
    "- learning rate을 증가함으로써 정확도가 높아졌다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cceabc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch : 0001 Avg cost = 0.924460\n",
      "Epoch : 0002 Avg cost = 0.444091\n",
      "Epoch : 0003 Avg cost = 0.358560\n",
      "Epoch : 0004 Avg cost = 0.310032\n",
      "Epoch : 0005 Avg cost = 0.277621\n",
      "Epoch : 0006 Avg cost = 0.253248\n",
      "Epoch : 0007 Avg cost = 0.234163\n",
      "Epoch : 0008 Avg cost = 0.219112\n",
      "Epoch : 0009 Avg cost = 0.205307\n",
      "Epoch : 0010 Avg cost = 0.194447\n",
      "Epoch : 0011 Avg cost = 0.184024\n",
      "Epoch : 0012 Avg cost = 0.175009\n",
      "Epoch : 0013 Avg cost = 0.167358\n",
      "Epoch : 0014 Avg cost = 0.160013\n",
      "Epoch : 0015 Avg cost = 0.153675\n",
      "최적화 완료!\n",
      "\n",
      "정확도 : 0.9426\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# 입력\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# First layer\n",
    "W1 = tf.Variable(tf.random_uniform([784, 256], -1., 1.))\n",
    "b1 = tf.Variable(tf.random_uniform([256], -1., 1.))\n",
    "L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# Second layer\n",
    "W2 = tf.Variable(tf.random_uniform([256, 256], -1., 1.))\n",
    "b2 = tf.Variable(tf.random_uniform([256], -1., 1.))\n",
    "L2 = tf.sigmoid(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "# output layer\n",
    "W3 = tf.Variable(tf.random_uniform([256, 10], -1., 1.))\n",
    "b3 = tf.Variable(tf.random_uniform([10], -1., 1.))\n",
    "model = tf.matmul(L2, W3) + b3\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 학습\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch_size = 100\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        total_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            _, cost_val = sess.run([optimizer, cost], feed_dict={X:batch_xs, Y:batch_ys})\n",
    "            total_cost += cost_val\n",
    "            \n",
    "        print('Epoch :', '%04d'%(epoch+1), 'Avg cost =', '{:3f}'.format(total_cost/total_batch))\n",
    "    print('최적화 완료!', end='\\n\\n')\n",
    "    \n",
    "    # 평가\n",
    "    is_correct = tf.equal(tf.argmax(model, 1), tf.math.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    print('정확도 :', sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb632e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
