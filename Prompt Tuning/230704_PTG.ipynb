{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTG Training code 분석\n",
    "작성자 : 설지우\n",
    "\n",
    "아래의 code는 Textbox 2.0의 PTG와 관련된 code를 이해를 위해 추출한 파일입니다.\n",
    "\n",
    "설정\n",
    "* github link: [https://github.com/RUCAIBox/TextBox](https://github.com/RUCAIBox/TextBox)\n",
    "* 수행서버: 서버3 GPU2 RTX 3090 single GPU\n",
    "* 파일 위치는 TextBox/230704 PTG.ipynb입니다.\n",
    "* dataset은 \"XSum\"으로 수행합니다. 부디, Textbox/dataset/xsum에 dataset을 미리 다운로드 받으셔야함\n",
    "* 이해를 위해 1개의 batch만을 추출하여 수행하였습니다. \n",
    "* 시작은 'run_textbox.py'부터 'trainer'까지 호출되는 함수들을 순서대로 정리한 것이니 참고만 해주세요. \n",
    "<br><br>\n",
    "\n",
    "순서\n",
    "1. Load Prompts: source_prompt.pth 파일 로드 \n",
    "2. Load XSum : XSum test dataset을 로드하고 1개의 batch로 추출 \n",
    "3. Load PTG model : PTG model의 내부 동작을 추출한 code입니다. (model의 forward 동작에 해당됨)<br>\n",
    "    3-1. PTG의 _process_prompt_tuning_input <br>\n",
    "    3-2. model()<br>\n",
    "    3-3. Loss function<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cnndm': tensor([[-0.5581,  0.5014, -0.2667,  ..., -0.3199, -0.4092, -0.9452],\n",
       "         [ 0.3267, -0.0185,  1.6073,  ..., -0.6338,  0.2770, -0.9944],\n",
       "         [-0.1004,  0.2631,  0.2974,  ..., -0.2703, -0.1962, -0.0020],\n",
       "         ...,\n",
       "         [-0.4019,  0.0385,  1.0486,  ...,  0.0838, -0.5052,  0.0451],\n",
       "         [-0.0628,  0.0940, -0.0373,  ..., -0.7181,  0.3422,  0.1762],\n",
       "         [ 0.0689,  0.5133,  0.2685,  ...,  0.6588,  0.9140,  0.7408]]),\n",
       " 'da': tensor([[-9.4889e-01, -3.9889e-01,  1.3212e+00,  ..., -1.2951e-01,\n",
       "           2.0233e-01,  2.5516e-01],\n",
       "         [ 1.1851e+00, -3.0833e-01,  7.8064e-01,  ...,  1.9099e-02,\n",
       "          -8.1817e-01, -1.1408e+00],\n",
       "         [-6.8315e-01, -2.8800e-01,  1.0065e+00,  ..., -7.6061e-02,\n",
       "          -1.1598e+00, -9.9347e-01],\n",
       "         ...,\n",
       "         [-2.4265e-01,  7.2915e-01,  2.9903e-01,  ...,  5.9689e-01,\n",
       "          -8.6180e-01, -4.8323e-01],\n",
       "         [ 5.9264e-01,  1.1957e-03,  7.2779e-01,  ..., -1.2124e+00,\n",
       "           7.4641e-01,  4.6784e-01],\n",
       "         [ 4.9494e-01, -5.2127e-01, -6.5174e-01,  ...,  2.4949e-01,\n",
       "           7.1435e-01,  8.7446e-04]]),\n",
       " 'dd': tensor([[-0.6275,  0.2855,  0.5409,  ..., -0.3211, -0.5893, -0.9229],\n",
       "         [ 0.4898, -0.4437,  0.9411,  ...,  0.4323,  0.3792, -1.1964],\n",
       "         [-0.0447, -0.0457,  0.7923,  ..., -0.1448, -0.8577, -0.4551],\n",
       "         ...,\n",
       "         [ 0.4916,  0.6995, -1.0121,  ..., -0.6491,  0.4493, -0.4717],\n",
       "         [-0.0807,  0.7062,  1.1356,  ..., -0.6056,  0.7002,  0.1467],\n",
       "         [ 0.0294,  0.0977, -0.1162,  ...,  0.3440,  0.4864, -0.8633]]),\n",
       " 'mn': tensor([[ 0.1126, -0.5964, -0.6305,  ..., -0.8309, -0.4669, -0.2146],\n",
       "         [ 1.8721, -0.2610,  0.2209,  ..., -0.4023, -0.0536, -1.9454],\n",
       "         [-0.4922, -0.4374,  0.5301,  ..., -0.0629, -0.6219, -1.1663],\n",
       "         ...,\n",
       "         [ 0.9392,  0.1661,  0.5023,  ...,  0.4050,  0.0415, -0.5489],\n",
       "         [ 0.3170, -0.2259,  0.1452,  ..., -0.5488,  0.3570,  0.1426],\n",
       "         [ 1.0970, -1.1439, -0.2818,  ...,  0.1852,  1.7039,  0.5661]]),\n",
       " 'msn': tensor([[-0.0630, -0.2443, -0.2984,  ..., -1.1609,  0.3433, -0.3456],\n",
       "         [ 0.0665, -0.1531, -0.0677,  ...,  0.3013, -0.0750, -0.8014],\n",
       "         [-0.5346,  0.3346,  0.1396,  ..., -0.1542, -0.7758, -0.4402],\n",
       "         ...,\n",
       "         [ 0.7226,  0.1195,  0.0275,  ..., -0.1138, -0.3159, -0.4297],\n",
       "         [ 0.1927, -0.4741,  0.4133,  ..., -0.3127, -0.1846,  0.2476],\n",
       "         [ 0.6453, -0.1984, -0.1466,  ...,  0.3042,  0.5509,  0.0901]]),\n",
       " 'mw': tensor([[-0.5068,  0.2801, -0.6242,  ...,  0.3681, -0.1849, -2.5296],\n",
       "         [ 0.5864,  0.3920, -0.2546,  ...,  0.7341, -0.5084, -0.1659],\n",
       "         [ 0.0782, -1.0856,  0.3447,  ..., -0.2676, -0.8160,  0.0690],\n",
       "         ...,\n",
       "         [ 1.0063,  0.4624,  1.1092,  ...,  0.2551, -0.4592, -0.0803],\n",
       "         [-0.1051,  0.3752,  0.6522,  ...,  0.1911,  0.3579, -0.0670],\n",
       "         [-0.3664, -0.6203, -0.5043,  ..., -0.2701,  0.5303,  0.3494]]),\n",
       " 'nr': tensor([[-0.6813, -0.5425,  0.1046,  ..., -0.1398, -0.5591, -0.3774],\n",
       "         [ 0.6940, -0.4416, -0.8099,  ..., -0.0043,  0.0536,  0.3557],\n",
       "         [-1.4746,  0.2711,  0.4076,  ..., -0.1464, -0.4345, -0.4889],\n",
       "         ...,\n",
       "         [-0.2156, -0.2441, -0.7110,  ..., -0.1826,  0.7684,  0.2274],\n",
       "         [ 0.2261,  0.2481,  0.6870,  ..., -0.7991,  0.0281, -0.0802],\n",
       "         [-0.0258, -0.2187,  0.0926,  ..., -0.0357, -0.3127,  0.1320]]),\n",
       " 'pc': tensor([[ 0.3771,  0.0454, -0.1098,  ..., -0.7998,  0.9884,  0.4053],\n",
       "         [ 0.1234,  1.0059, -0.4667,  ..., -0.0711, -0.3662,  0.2656],\n",
       "         [-0.7045, -0.0858,  1.0409,  ..., -0.3787, -1.6326, -0.6629],\n",
       "         ...,\n",
       "         [-0.4976, -0.0777, -0.1398,  ..., -0.1074, -0.9009, -0.6133],\n",
       "         [ 0.3913, -0.4850,  0.0591,  ..., -1.4067,  0.3170, -0.3534],\n",
       "         [-0.1243, -0.3804, -0.6440,  ..., -0.5986,  1.6229,  0.0237]]),\n",
       " 'quora': tensor([[-0.1040, -0.1116, -0.4819,  ..., -0.4798,  0.0852, -0.2904],\n",
       "         [ 1.1764,  0.6514,  0.1224,  ...,  0.0655,  0.0891, -1.0207],\n",
       "         [-0.0929,  0.2250,  0.9940,  ..., -0.4628, -0.2217, -0.5807],\n",
       "         ...,\n",
       "         [ 0.8869,  0.5676,  0.5024,  ...,  0.4265,  0.2007, -0.2157],\n",
       "         [ 0.7690, -0.1253,  0.4945,  ..., -0.5311,  0.1789,  0.0433],\n",
       "         [ 1.0856,  0.0876, -0.6486,  ...,  0.3485,  0.4612,  0.5136]]),\n",
       " 'squad': tensor([[-0.4166, -0.1726, -0.8167,  ..., -0.6554, -0.0383, -0.7906],\n",
       "         [-0.2325, -0.8249,  0.5206,  ..., -1.0618,  0.4780, -0.6072],\n",
       "         [-0.4961, -0.1151,  0.7405,  ..., -0.5986, -1.0617, -0.4603],\n",
       "         ...,\n",
       "         [-0.4455, -0.4636, -0.4180,  ...,  0.4117,  0.1873, -0.8304],\n",
       "         [ 0.5472,  0.2171,  0.0619,  ..., -1.0280,  0.1316,  0.3724],\n",
       "         [ 0.2330, -0.3975, -0.6579,  ...,  0.0800,  0.7506,  0.0058]]),\n",
       " 'tc': tensor([[-0.3880,  0.9463, -0.2782,  ..., -0.3439,  0.5958, -0.6526],\n",
       "         [ 0.4761,  0.3302,  0.0547,  ..., -0.2510,  0.0937, -0.0220],\n",
       "         [-0.4069,  0.2057, -0.1828,  ..., -0.2595, -0.6273, -1.0275],\n",
       "         ...,\n",
       "         [-0.2214,  0.3725, -0.1570,  ..., -0.0730,  0.0163,  0.8003],\n",
       "         [ 0.9878,  0.2299,  0.6498,  ..., -0.2694, -0.0129, -0.4937],\n",
       "         [ 0.0064,  0.0477,  0.5079,  ...,  0.3465,  0.1677, -0.0722]]),\n",
       " 'wiki': tensor([[-0.5284,  0.5155,  1.2064,  ..., -0.5063,  0.0144, -0.0136],\n",
       "         [ 0.2471,  0.9813, -0.1003,  ..., -0.2245,  0.0096, -0.6364],\n",
       "         [-0.5345, -0.0297,  0.6091,  ..., -0.2512, -0.2155, -0.8238],\n",
       "         ...,\n",
       "         [-0.0538,  1.1131,  0.0537,  ..., -0.0166,  0.1000, -0.1982],\n",
       "         [ 0.5482,  0.6870,  0.8790,  ..., -0.3778,  0.8051,  0.1082],\n",
       "         [ 0.3005,  0.1416, -0.2720,  ...,  0.3278,  1.1223,  1.1045]]),\n",
       " 'wp': tensor([[ 0.3206,  0.5982,  0.1904,  ..., -0.7331,  0.1918, -0.1147],\n",
       "         [-0.5028,  0.0297,  1.4839,  ...,  0.5224,  0.2757, -0.9576],\n",
       "         [-1.7517, -0.5212, -0.3830,  ..., -0.3541, -0.7779, -0.6142],\n",
       "         ...,\n",
       "         [ 0.4518,  0.0983, -0.1795,  ...,  0.4485, -0.6298,  0.5147],\n",
       "         [ 0.8403,  1.0624,  0.2014,  ..., -1.2431,  0.4707,  0.3187],\n",
       "         [ 0.7577, -0.3613, -0.0176,  ...,  0.2932,  0.5870,  0.5924]]),\n",
       " 'xsum': tensor([[-1.1669, -0.4849,  0.7534,  ..., -0.5900, -1.1410, -0.5029],\n",
       "         [-0.0253,  0.8711,  0.1534,  ...,  0.4280,  0.3053, -0.2263],\n",
       "         [-0.0712,  0.0717,  0.2654,  ...,  0.1562,  0.2077, -0.2932],\n",
       "         ...,\n",
       "         [-0.2866, -0.0156,  0.4145,  ...,  0.2818, -0.6070,  0.3349],\n",
       "         [ 0.5389, -0.3206, -0.3237,  ..., -0.9140,  0.4760,  0.3233],\n",
       "         [ 0.0227, -0.4547, -0.0120,  ...,  0.0266,  0.6331,  0.1069]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "source_task_set = {\n",
    "    'cross-task1': ['squad', 'wiki', 'quora', 'wp', 'cnndm'],\n",
    "    'cross-task2': ['squad', 'wiki', 'quora', 'wp', 'pc'],\n",
    "    'cross-dataset1': ['msn', 'mn', 'nr'],\n",
    "    'cross-dataset2': ['tc', 'da', 'mw'],\n",
    "}\n",
    "\n",
    "prompt_source = torch.load('prompt_source.pth')\n",
    "prompt_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_task = source_task_set['cross-dataset2']\n",
    "task_embedding = [prompt_source[task] for task in source_task]\n",
    "task_embedding = torch.stack(task_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_embedding : torch.Size([3, 200, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(f'task_embedding : {task_embedding.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load XSum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textbox.data.misc import load_data, _pad_sequence\n",
    "\n",
    "data_path = 'dataset/xsum'\n",
    "source_filename = os.path.join(data_path, f'test.src')\n",
    "target_filename = os.path.join(data_path, f'test.tgt')\n",
    "\n",
    "source_text = load_data(source_filename, max_length=0)\n",
    "source_length = 800        # 원래는 1024지만 length 계산하기 시간이 부족해서 임의로 설정했음\n",
    "target_text = load_data(target_filename, max_length=0)\n",
    "target_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _process_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Summarize:'\n",
    "suffix = \"\"\n",
    "\n",
    "prefix_ids = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_ids = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "source_max_length = source_length - tokenizer.num_special_tokens_to_add() - len(prefix_ids) - len(suffix_ids)\n",
    "target_max_length = target_length - tokenizer.num_special_tokens_to_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix_ids : [38182, 3916, 2072, 35]\n"
     ]
    }
   ],
   "source": [
    "print(f'prefix_ids : {prefix_ids}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1185 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "index = [0,2,8,3]\n",
    "source_ids = []\n",
    "ids = tokenizer(\n",
    "        source_text,\n",
    "        add_special_tokens=False,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "    )['input_ids']\n",
    "source_ids.extend(ids)\n",
    "source_ids = source_ids[:10]\n",
    "\n",
    "after_src_ids = []\n",
    "for ids in source_ids:\n",
    "    ids = ids[:source_length] \n",
    "    ids = prefix_ids + ids + suffix_ids\n",
    "    ids = tokenizer.build_inputs_with_special_tokens(ids)\n",
    "    after_src_ids.append(torch.tensor(ids, dtype=torch.long))\n",
    "\n",
    "target_ids = []\n",
    "after_tgt_ids = []\n",
    "target_ids = tokenizer(\n",
    "    text_target=target_text,\n",
    "    add_special_tokens=False,\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=False   \n",
    ")['input_ids']\n",
    "target_ids = target_ids[:10]\n",
    "for ids in target_ids:\n",
    "    ids = ids[:target_length]\n",
    "    ids = tokenizer.build_inputs_with_special_tokens(ids)\n",
    "    after_tgt_ids.append(torch.tensor(ids, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textbox.data.misc import load_data, _pad_sequence\n",
    "\n",
    "batch = {}\n",
    "cur_source_text = []\n",
    "cur_source_ids = []\n",
    "cur_source_mask = []\n",
    "cur_source_length = []\n",
    "cur_target_text = []\n",
    "cur_source_padding_side = 'right'\n",
    "\n",
    "for idx in [0,2,8,3]:\n",
    "    cur_source_text.append(source_text[idx])\n",
    "    cur_source_ids.append(after_src_ids[idx])\n",
    "    cur_source_mask.append(torch.ones(len(after_src_ids[idx]), dtype=torch.long))\n",
    "    cur_source_length.append(len(after_src_ids[idx]))\n",
    "    cur_target_text.append(target_text[idx])\n",
    "    \n",
    "batch[\"source_text\"] = cur_source_text\n",
    "batch[\"source_ids\"] = _pad_sequence(cur_source_ids, tokenizer.pad_token_id, cur_source_padding_side)\n",
    "batch[\"source_mask\"] = _pad_sequence(cur_source_mask, 0, cur_source_padding_side)\n",
    "batch[\"source_length\"] = torch.tensor(cur_source_length, dtype=torch.long)\n",
    "batch[\"target_text\"] = cur_target_text\n",
    "\n",
    "cur_target_ids = []\n",
    "for idx in [0,2,8,3]:\n",
    "    cur_target_ids.append(after_tgt_ids[idx])\n",
    "batch['target_ids'] = _pad_sequence(cur_target_ids, -100, tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_text': ['Fast forward about 20 years, and it\\'s fair to say he has done just that. The business he runs, Frasers Hospitality, is one of the world\\'s biggest providers of high-end serviced apartments. Its 148 properties span about 80 capital cities, as well as financial hubs across Europe, Asia, the Middle East and Africa. But it almost didn\\'t get off the ground. When Mr Choe was appointed to launch and lead the company, Asia was booming; the tiger economies of Hong Kong, South Korea, Taiwan and Singapore were expanding rapidly. But as Frasers prepared to open its first two properties in Singapore, the Asian financial crisis hit. It was 1997. Currencies went into freefall. Suddenly, people were losing their jobs and stopped travelling. Mr Choe recalls asking staff if they really wanted to continue working with the firm, because when the properties opened they might not get paid. \"It was really that serious,\" he says. \"I remember tearing up because they said \\'let\\'s open it, let\\'s open it whether you can pay us or not\\'.\" Survival, Mr Choe admits, came through a bit of luck, and the misfortune of others. He had convinced the board at parent firm, property group Frasers Centrepoint, to open serviced apartments rather than hotels - partly because getting planning permission in Singapore was easier. But he also sensed it was a big, untapped market. And at the time of the crisis, it proved to be exactly what customers wanted. \"As we were going through this difficult patch, there were protests and riots in Jakarta,\" he says. \"A lot of companies like Microsoft called up looking for rooms for their staff because they were moving out of Jakarta.\" Frasers\\' 412 apartments were quickly in demand. Occupancy soon hit 70%, and then 90%. Explaining the popularity of serviced apartments, Mr Choe says that if people are staying somewhere for just a few days, they happily stay in hotels, but if they are going to be somewhere for one month to eight months, the walls of hotel rooms \"close in on you\". But now, Mr Choe, 57, faces new challenges - the travel tastes of millennials and the disruptive nature of Airbnb. \"The way to tackle Airbnb is not to ignore it. I will never underestimate Airbnb,\" he says. There\\'s been no significant impact on Frasers yet. Big corporations still prefer to put employees in big service apartments, he says, because they can guarantee a level of safety and security. But that is likely to change, Mr Choe admits. \"I have two daughters who to my chagrin use Airbnb,\" he says. \"We took a family trip to Florence and I stayed in this wonderful boutique hotel, but paid a bundle for it. \"When my daughter joined us, she said, \\'I\\'m just staying next door and paying about 80 euros\\'. We paid about 330 euros. \"I asked why they stayed at Airbnb. They say \\'it\\'s like a surprise, it\\'s part of the adventure\\'.\" And so now, Mr Choe wants to bring some of that vibrancy to Frasers. While neutral colours, beige curtains and dark wooden chairs dominate its more traditional apartments, many customers want something different, and this is shaping Fraser\\'s strategy. In 2015 it bought Malmaison Hotel du Vin, a UK hotel group that specialises in developing heritage properties into upscale boutique hotels. That has taken them beyond financial centres, including to Shakespeare\\'s hometown of Stratford-upon-Avon. Or, an intrepid traveller with $500 (Â£325) to spend could have a night in a converted medieval prison in Oxford. And Frasers has launched the Capri sub-brand - whose website promises \"inspiring art and inspirational tech\". On a day-to-day basis Mr Choe says he still draws on his experience as a young man, who - having been given a scholarship by the Shangri-La hotel group to study at Cornell University in the US - came back to Asia to learn about the hospitality industry. \"They put me in every department conceivable. I remember one of the toughest jobs I had was in the butchery. I had to carve an entire cow. For one month, I could not eat meat. \"I\\'m thankful for those experiences. When you step into a hotel, you immediately pick up what works and what doesn\\'t work. \"When I see the check-in staff walking more than three steps, I know the counter is set up wrong. \"It\\'s like a cockpit. Can you imagine if the pilot had to turn around when he flies?\" More The Boss features, which every week profile a different business leader from around the world: The \\'diva of divorce\\' for the world\\'s super rich The snacks boss with an appetite for success Taking his own path: The world\\'s leading maze designer Mr Choe adds that loyalty is very important to him, and he remains tremendously grateful to staff who have stayed with him. \"I will always respect and remember those who gave up their jobs to join me,\" he says. This loyalty is something that Mr Choe has earned, according to Donald MacLaurin, associate professor at Singapore Institute of Technology, and specialist in the hospitality sector. Mr MacLaurin points out that Mr Choe introduced a five-day working week, in a part of the world where six days is common, thereby showing \"a focus on quality of life issues for employees\". The associate professor adds says the early success of the business was remarkable given the timing of its launch. Fast forward to today and the company is now on track to operate 30,000 serviced apartments globally by 2019. That success, say Mr Choe\\'s admirers, should make him something of a visionary. Follow The Boss series editor Will Smale on Twitter.',\n",
       "  'The Sunday Times says the missile veered off course during a test in June last year - weeks before the Commons voted to spend £40bn renewing Trident. Questioned by Andrew Marr, the PM refused to say four times if she had known about the test ahead of the vote. The SNP\\'s Nicola Sturgeon called for a \"full disclosure\" of what happened. According to the Sunday Times, an unarmed Trident II D5 missile veered off in the wrong direction towards the US - instead of towards Africa - when it was launched from a British submarine off the coast of Florida. In July - days after Mrs May had become prime minister - MPs voted overwhelmingly in favour of replacing Trident. During the debate, Mrs May told MPs it would be \"an act of gross irresponsibility\" for the UK to abandon its nuclear weapons. MPs backed its renewal by 472 votes to 117. However, all 52 SNP MPs voted against it - as did Labour leader Jeremy Corbyn. When asked on the BBC\\'s Andrew Marr Show whether she had known then that a misfire had happened, Mrs May said: \"I have absolute faith in our Trident missiles. \"When I made that speech in the House of Commons, what we were talking about was whether or not we should renew our Trident.\" She was asked a further three times - but did not answer the questions. The Ministry of Defence did not give details of the test process but said it had been a success. Scottish First Minister, Mrs Sturgeon - a long-standing opponent of Trident, whose submarines are based at Faslane, on the River Clyde - said the apparent misfire was a \"hugely serious issue\". She tweeted: \"There should be full disclosure of what happened, who knew what/when, and why the House of Commons wasn\\'t told.\" Meanwhile, Mr Corbyn said the reports called for \"a serious discussion\". He told Sky News: \"It\\'s a pretty catastrophic error when a missile goes in the wrong direction, and while it wasn\\'t armed, goodness knows what the consequences of that could have been.\" Nia Griffith, the shadow defence secretary, said it was \"completely unacceptable\" that Mrs May had \"side-stepped\" questions. She called for the prime minister to give \"a full explanation\" to Parliament on Monday. Admiral Lord West, the Labour peer and ex-Royal Navy officer, said it was \"bizarre and stupid\" to not tell anyone about the test. The Campaign for Nuclear Disarmament (CND) described reports of a misfire as a \"very serious failure\". \"There\\'s absolutely no doubt that this would have impacted on the debate in Parliament on Trident replacement,\" its general secretary Kate Hudson said. A statement issued by both Downing Street and the MoD said the capability and effectiveness of Trident was \"unquestionable\". \"In June the Royal Navy conducted a routine, unarmed Trident missile test launch from HMS Vengeance, as part of an operation which is designed to certify the submarine and its crew. \"Vengeance and her crew were successfully tested and certified, allowing Vengeance to return into service. We have absolute confidence in our independent nuclear deterrent.\" The Sunday Times says the test fire was launched from HMS Vengeance. It says the Trident II D5 missile was intended to be fired 5,600 miles (9,012 km) to a sea target off the west coast of Africa but veered towards the US. The cause remains top secret, the paper says, but it quotes a senior naval source as saying the missile suffered an in-flight malfunction after launching out of the water. HMS Vengeance, one of the UK\\'s four Vanguard-class submarines, returned to sea for trials in December 2015 after a £350m refit, which included the installation of new missile launch equipment and upgraded computer systems. According to the Sunday Times, it is expected that Defence Secretary Michael Fallon will be called to the Commons to answer questions from MPs. BBC defence correspondent Jonathan Beale said while the MoD has described the test as a success for the crew and the boat, it has not denied the report that the missile itself might have veered off course. In the past the MoD has issued a press release and video of successful tests but its silence on this occasion has raised questions as to whether any fault was deliberately kept quiet ahead of the key vote, our correspondent added. The Trident system was acquired by the Thatcher government in the early 1980s as a replacement for the Polaris missile system, which the UK had possessed since the 1960s. Trident came into use in the 1990s. There are three parts to it - submarines, missiles and warheads. Although each component has years of use left, they cannot last indefinitely. The current generation of four submarines would begin to end their working lives some time in the late 2020s. A guide to the Trident debate',\n",
       "  '\"Fracking\" involves pumping water and chemicals into shale rock at pressure. The joint report from the Royal Society and Royal Academy of Engineering say the technique is safe if firms follow best practice and rules are enforced. Exploratory fracking is being mooted in at least seven sites around the UK. The report was commissioned by the government\\'s chief scientist, Sir John Beddington, following the decision last year to halt the UK\\'s most advanced project, in Lancashire, after fracking caused small earth tremors. \"Our main conclusions are that the environmental risks of hydraulic fracturing for shale can can be safely managed provided there is best practice observed and provided it\\'s enforced through strong regulation,\" said the report\\'s chair, Prof Robert Mair from Cambridge University. \"The UK regulatory system is up to the job for the present very small scale exploration activities, but there would need to be strengthening of the regulators if the government decides to proceed with more shale gas extraction, particularly at the production stage,\" he told BBC News. The report contains 10 top-line recommendations for strengthening regulations, including: Fracking, or hydraulic fracturing, involves drilling down into shale formations and fracturing the rock using explosives or hydraulic pressure. Water is pumped in containing sand particles that prop open the cracks in the shale, allowing the gas out, and a tiny amount of chemicals. In the US, concerns have been raised that the gas or the chemicals can enter drinking water supplies. However, the report concludes that gas contamination should not be a problem - the risk is \"very low\" - provided that fracking takes place at a depth of many hundreds of metres, a long way below the level of aquifers, and that the wells are properly constructed. Each well is lined with layers of steel and cement; and if this stays intact, the scientists conclude, gas leakage should not be a problem. They also say that with good management of waste water, chemical contamination should be avoided; they criticise the US practice of leaving it in open ponds, which would not be permitted in the UK. The report says the risk that fracking will generate significant seismic events is also small. The actual explosions are far too small to be noticed at the surface, said Prof Zoe Shipton from the University of Strathclyde. \"If the fluid moves into existing faults in the rock that are close to slipping anyway, you\\'ll bring that slippage forward in time,\" she said. \"But the Magnitude 2.3 event in Blackpool last year - that is like a lorry going past your house - in fact the British Geological Survey can\\'t measure below Magnitude 2 in towns because of the traffic.\" The European industry is in its infancy compared with the US, and governments\\' views on the technology are mixed. Fracking has been banned in France, but countries such as Poland are taking a strong interest. UK ministers see shale gas as a way of cheaply bridging the transition to a low-carbon fuel mix, despite calculations showing that widespread adoption without carbon capture and storage (CCS) technology would mean future UK governments missing their legally-binding greenhouse gas emission reduction targets. Prof Mair said climate impacts were outside his remit, but a detailed look at the issue should happen at some stage. \"The Royal Society and Royal Academy of Engineering believe this is an issue that needs to be addressed,\" he said. \"It\\'s a very wide issue - energy policy, CO2 reduction, all of that needs to be looked at - and gas in general is part of that.\" Follow Richard             on Twitter',\n",
       "  'A spokesman for Palm Beach Gardens police in Florida confirmed to the BBC they were investigating a fatal crash involving the Grand Slam champion. A man was taken to hospital after the accident on 9 June and died two weeks later from his injuries, he said. According to TMZ, which broke the story, police believe the seven-time Grand Slam champion was at fault. But a lawyer for Williams said it was an \"unfortunate accident\". The man who died, Jerome Barson, was travelling with his wife who was driving their vehicle through an intersection when the accident happened. Williams\\' car suddenly darted into their path and was unable to clear the junction in time due to traffic jams, according to witness statements in a police report obtained by US media. Mrs Barson was also taken to hospital but survived. \"[Williams] is at fault for violating the right of way of [the other driver],\" the report said, adding that there were no other factors like drugs, alcohol or mobile phone distractions. The 37-year-old tennis star reportedly told police she did not see the couple\\'s car and she was driving slowly. Police spokesman Major Paul Rogers said police were investigating whether the incident was connected to Mr Barson\\'s death. Williams\\' lawyer Malcolm Cunningham told CNN in a statement: \"Ms Williams entered the intersection on a green light. The police report estimates that Ms Williams was travelling at 5mph when Mrs Barson crashed into her. \"Authorities did not issue Ms Williams with any citations or traffic violations. This is an unfortunate accident and Venus expresses her deepest condolences to the family who lost a loved one.\" Next week, Williams is due to play at Wimbledon in London, where she is seeded 10th.'],\n",
       " 'source_ids': tensor([[    0, 38182,  3916,  ...,     5, 38724,     2],\n",
       "         [    0, 38182,  3916,  ...,  3713,   495,     2],\n",
       "         [    0, 38182,  3916,  ...,     1,     1,     1],\n",
       "         [    0, 38182,  3916,  ...,     1,     1,     1]]),\n",
       " 'source_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'source_length': tensor([806, 806, 733, 347]),\n",
       " 'target_text': ['On the first day in his new job, Choe Peng Sum was given a fairly simple brief: \"Just go make us a lot of money.\"',\n",
       "  \"Theresa May is coming under pressure to say whether she knew about a reported misfire of the UK's nuclear weapons system before a crucial Commons vote.\",\n",
       "  'A gas extraction method which triggered two earth tremors near Blackpool last year should not cause earthquakes or contaminate water but rules governing it will need tightening, experts say.',\n",
       "  'US tennis star Venus Williams has been involved in a car accident that led to the death of a 78-year-old man.'],\n",
       " 'target_ids': tensor([[    0,  4148,     5,    78,   183,    11,    39,    92,   633,     6,\n",
       "            732,  3540, 34597,  9430,    21,   576,    10,  5342,  2007,  4315,\n",
       "             35,    22,  6785,   213,   146,   201,    10,   319,     9,   418,\n",
       "             72,     2,  -100,  -100,  -100,  -100],\n",
       "         [    0,   133, 19716,   392,    16,   567,   223,  1164,     7,   224,\n",
       "            549,    79,  1467,    59,    10,   431,  3834,  7051,     9,     5,\n",
       "            987,    18,  1748,  2398,   467,   137,    10,  4096, 10271,   900,\n",
       "              4,     2,  -100,  -100,  -100,  -100],\n",
       "         [    0,   250,  1123, 23226,  5448,    61,  7544,    80,  6872, 33704,\n",
       "            994,   583,  1378, 10416,    94,    76,   197,    45,  1303, 20396,\n",
       "             50, 43337, 13014,   514,    53,  1492,  8182,    24,    40,   240,\n",
       "          12872,     6,  2320,   224,     4,     2],\n",
       "         [    0,  3048,  5919,   999, 18168,  1604,    34,    57,   963,    11,\n",
       "             10,   512,  3213,    14,   669,     7,     5,   744,     9,    10,\n",
       "           7004,    12,   180,    12,   279,   313,     4,     2,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load PTG model\n",
    "\n",
    "* [AbstractModel](https://github.com/RUCAIBox/TextBox/blob/2.0.0/textbox/model/abstract_model.py) ==(상속)==> [Pretrained_Models](https://github.com/RUCAIBox/TextBox/blob/2.0.0/textbox/model/pretrained_models.py) ==(상속)==> [PTG](https://github.com/RUCAIBox/TextBox/blob/2.0.0/textbox/model/ptg.py) \n",
    "* model의 forward 부분을 이해를 위해 재구성한 code임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "arg_model = 'PTG'\n",
    "arg_dataset = 'xsum'\n",
    "args_config_files = []\n",
    "self_task_num = 3\n",
    "self_head_num = 16\n",
    "self_head_dim = 64\n",
    "self_scaling = self_head_dim**-0.5\n",
    "self_embedding_size = 1024\n",
    "self_k_proj = nn.Linear(self_embedding_size, self_embedding_size)  # in_feature x out_feature\n",
    "self_v_proj = nn.Linear(self_embedding_size, self_embedding_size)\n",
    "self_q_proj = nn.Linear(self_embedding_size, self_embedding_size)\n",
    "self_out_proj = nn.Linear(self_embedding_size, self_embedding_size)\n",
    "self_task_key = nn.Embedding(self_task_num+1, self_embedding_size)  # https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-large-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from accelerate import Accelerator\n",
    "from textbox.config.configurator import Config\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "module_path = '.'.join(['textbox.model', 'PTG'.lower()])\n",
    "if importlib.util.find_spec(module_path, __name__):\n",
    "    model_module = importlib.import_module(module_path, __name__)\n",
    "    model_class = getattr(model_module, 'PTG')\n",
    "self_model = model_class\n",
    "\n",
    "config = Config('PTG', 'xsum', [], {})\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=config['find_unused_parameters'])\n",
    "accelerator = Accelerator(\n",
    "            gradient_accumulation_steps=config['accumulation_steps'], kwargs_handlers=[ddp_kwargs]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. PTG의 _process_prompt_tuning_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "self_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "self_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embeds : torch.Size([4, 806, 1024])\n"
     ]
    }
   ],
   "source": [
    "# process_forward_inputs\n",
    "inputs = {\n",
    "    'input_ids': batch['source_ids'],\n",
    "    'attention_mask': batch['source_mask'],\n",
    "    'labels': batch['target_ids']\n",
    "}\n",
    "\n",
    "# process_prompt_tuning_input\n",
    "input_ids = inputs['input_ids']\n",
    "batch_size = input_ids.size(0)\n",
    "input_embeds = self_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "print(f'input_embeds : {input_embeds.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* repeat : https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(text):\n",
    "    encoding_dict = bert_tokenizer(text, max_length=bert_tokenizer.model_max_length, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoding_dict['input_ids']\n",
    "    attn_masks = encoding_dict['attention_mask']\n",
    "    output = bert_model(input_ids, attn_masks)['last_hidden_state'] # b, l, dim\n",
    "    print(f'BERT-large output : {output.shape}')\n",
    "    \n",
    "    hidden_state = output*attn_masks.unsqueeze(-1)\n",
    "    print(f'BERT-large hidden state : {hidden_state.size()}')\n",
    "    embedding = hidden_state.sum(dim=1)/attn_masks.sum(dim=1).unsqueeze(-1)\n",
    "    return embedding.detach()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* key: a learnable clustr key == a learnable prompt key (batch size, task num, dim)\n",
    "* input_query: instance-level query (batch size, 1, dim)\n",
    "* value: source prompt로 부터 얻은 task-specific knowledge (=reprsentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ KEY ------------------\n",
      "self.task_key : torch.Size([4, 1024])\n",
      "task_key : torch.Size([4, 4, 1024])\n",
      "key : torch.Size([4, 3, 1024])\n",
      "--------------------------------------------\n",
      "\n",
      "\n",
      "------------------ VALUE ------------------\n",
      "self_v_proj(task_embedding) : torch.Size([3, 200, 1024])\n",
      " => reshape : torch.Size([3, 204800])\n",
      " => repeat : torch.Size([4, 3, 204800])\n",
      "--------------------------------------------\n",
      "\n",
      "\n",
      "------------------ QUERY ------------------\n",
      "task-level query (task_query): torch.Size([4, 1, 1024])\n",
      "\n",
      "BERT-large output : torch.Size([4, 512, 1024])\n",
      "BERT-large hidden state : torch.Size([4, 512, 1024])\n",
      "instance-level query (input_query): torch.Size([4, 1, 1024])\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## KEY\n",
    "print('\\n------------------ KEY ------------------')\n",
    "task_key = self_task_key.weight.repeat(batch_size, 1, 1)  # b, tn+1, dim\n",
    "\n",
    "print(f'self.task_key : {self_task_key.weight.size()}')\n",
    "print(f'task_key : {task_key.size()}')\n",
    "\n",
    "# key: a learnable cluster key && a learnable prompt key\n",
    "key = self_k_proj(task_key[:, :-1])   # (b, tn (3), dim) 0번째~2번째\n",
    "print(f'key : {key.size()}')\n",
    "print('--------------------------------------------', end='\\n\\n')\n",
    "\n",
    "## VALUE\n",
    "value = self_v_proj(task_embedding).reshape(self_task_num,-1).repeat(batch_size, 1, 1)\n",
    "print(f'\\n------------------ VALUE ------------------')\n",
    "print(f'self_v_proj(task_embedding) : {self_v_proj(task_embedding).size()}')\n",
    "print(f' => reshape : {self_v_proj(task_embedding).reshape(self_task_num,-1).size()}')\n",
    "print(f' => repeat : {value.size()}')\n",
    "print('--------------------------------------------', end='\\n\\n')\n",
    "\n",
    "## QUERY\n",
    "print('\\n------------------ QUERY ------------------')\n",
    "# task-level query\n",
    "task_query = self_q_proj(task_key[:, -1:])  # (b, 1, dim) 3번째\n",
    "print(f'task-level query (task_query): {task_query.size()}', end='\\n\\n')\n",
    "\n",
    "# instance-level query\n",
    "input_query = sentence_embedding(batch['source_text']).unsqueeze(1)     # b, l, e\n",
    "print(f'instance-level query (input_query): {input_query.size()}')\n",
    "print('--------------------------------------------', end='\\n\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHA (Multi-Head Attention)\n",
    "\n",
    "* 역할: Query, Key, Value에 대하여 여러개의 Attention을 동시에 병렬적으로 수행\n",
    "* Multi-Head Attention을 수행하는 이유\n",
    "    * 하나의 단어가 여러가지 attention 값을 가질 수 있으므로 다수의 문맥 정보를 포함하기 위함\n",
    "* Attention을 수행하는 이유: 문맥에 따라 집중할 단어를 결정하는 방식 = 문맥 안에서 attention을 두도록 한다 = 문맥을 파악한다는 의미랑 같을 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def MHA(query, key, value):\n",
    "    \"\"\"\n",
    "        Transformer의 Multi-Head Self Attention과 동일함\n",
    "    \n",
    "        key: (4,3,1024)\n",
    "        query: \n",
    "            - instance-level query: (4,1,1024) \n",
    "            - task-level query: (4,1,1024)\n",
    "        value: (4,3,204800)\n",
    "    \"\"\"\n",
    "    print(f'-------------------------- MHA -------------------------')\n",
    "    batch_size = key.size(0)\n",
    "    \n",
    "    # (4,1,16,64) => (4,16,1,64) => (64,1,64) (b*h,1,d)\n",
    "    query = query.reshape(batch_size, -1, self_head_num, self_head_dim).transpose(1,2).reshape(batch_size*self_head_num, -1, self_head_dim)\n",
    "    print(f'query: {query.shape}')\n",
    "\n",
    "    # (4,3,16,64) => (4,16,3,64) => (64,3,64) (b*h,tn,d)\n",
    "    key = key.reshape(batch_size, -1, self_head_num, self_head_dim).transpose(1,2).reshape(batch_size*self_head_num, -1, self_head_dim)\n",
    "    print(f'key: {key.shape}')\n",
    "    \n",
    "    # (4,3,200,16,64) => (4,16,3,200,64) => (64,3,12800) (b*h,tn,pl*d)\n",
    "    value = value.reshape(batch_size, self_task_num, -1, self_head_num, self_head_dim).permute(0,3,1,2,4).reshape(batch_size*self_head_num, self_task_num,-1)\n",
    "    print(f'value: {value.shape}')\n",
    "    \n",
    "    # bmm: https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
    "    # Query와 Key 내적 유사도 구함 => scaling 수행\n",
    "    attn_weights = torch.bmm(query, key.transpose(1,2))*self_scaling  # (64,1,3) (b*h,1,tn)\n",
    "    print(f'Query x Key : {attn_weights.shape}')\n",
    "    \n",
    "    # softmax 취하여 가중치 얻음\n",
    "    attn_probs = F.dropout(attn_weights, p=0.1)\n",
    "    attn_probs = F.softmax(attn_probs, dim=1)   \n",
    "    \n",
    "    # 각 가중치와 Value 곱함\n",
    "    attn_output = torch.bmm(attn_probs, value)  # (64,1,12800) (b*h,1,pl*dim)\n",
    "    print(f'=> attention output : {attn_output.size()}')  \n",
    "    \n",
    "    # prompt embedding으로 형태로 변환\n",
    "    # (b*h, pl*dim) => (4,16,200,64) (b,h,pl,dim)\n",
    "    prompt_embedding = attn_output.squeeze(1).reshape(batch_size, self_head_num, -1, self_head_dim) \n",
    "    print(f'prompt_embedding : {prompt_embedding.size()}', end=' => ')\n",
    "    \n",
    "    # Concat 수행\n",
    "    # (4,200,16,64) => (4,200,1024)\n",
    "    prompt_embedding = prompt_embedding.transpose(1,2).reshape(batch_size, -1, self_embedding_size)\n",
    "    print(f'{prompt_embedding.shape}', end=' => ')\n",
    "    \n",
    "    prompt_embedding = self_out_proj(prompt_embedding)\n",
    "    print(f'{prompt_embedding.shape}')\n",
    "    print('----------------------------------------------------------------------------', end='\\n\\n')\n",
    "    return prompt_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation (4) 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- MHA -------------------------\n",
      "query: torch.Size([64, 1, 64])\n",
      "key: torch.Size([64, 3, 64])\n",
      "value: torch.Size([64, 3, 12800])\n",
      "Query x Key : torch.Size([64, 1, 3])\n",
      "=> attention output : torch.Size([64, 1, 12800])\n",
      "prompt_embedding : torch.Size([4, 16, 200, 64]) => torch.Size([4, 200, 1024]) => torch.Size([4, 200, 1024])\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "task_level_MHA : torch.Size([4, 200, 1024])\n"
     ]
    }
   ],
   "source": [
    "# task-level query / key / value\n",
    "task_level_MHA = MHA(task_query, key, value)\n",
    "print(f'task_level_MHA : {task_level_MHA.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- MHA -------------------------\n",
      "query: torch.Size([64, 1, 64])\n",
      "key: torch.Size([64, 3, 64])\n",
      "value: torch.Size([64, 3, 12800])\n",
      "Query x Key : torch.Size([64, 1, 3])\n",
      "=> attention output : torch.Size([64, 1, 12800])\n",
      "prompt_embedding : torch.Size([4, 16, 200, 64]) => torch.Size([4, 200, 1024]) => torch.Size([4, 200, 1024])\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "instance_level_MHA : torch.Size([4, 200, 1024])\n"
     ]
    }
   ],
   "source": [
    "# instance-level query / key / value\n",
    "instance_level_MHA = MHA(input_query, key, value)\n",
    "print(f'instance_level_MHA : {instance_level_MHA.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.5\n",
    "prompt_embeds = lam*task_level_MHA + (1-lam)*instance_level_MHA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 inputs : {'input_ids': tensor([[    0, 38182,  3916,  ...,     5, 38724,     2],\n",
      "        [    0, 38182,  3916,  ...,  3713,   495,     2],\n",
      "        [    0, 38182,  3916,  ...,     1,     1,     1],\n",
      "        [    0, 38182,  3916,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    0,  4148,     5,    78,   183,    11,    39,    92,   633,     6,\n",
      "           732,  3540, 34597,  9430,    21,   576,    10,  5342,  2007,  4315,\n",
      "            35,    22,  6785,   213,   146,   201,    10,   319,     9,   418,\n",
      "            72,     2,  -100,  -100,  -100,  -100],\n",
      "        [    0,   133, 19716,   392,    16,   567,   223,  1164,     7,   224,\n",
      "           549,    79,  1467,    59,    10,   431,  3834,  7051,     9,     5,\n",
      "           987,    18,  1748,  2398,   467,   137,    10,  4096, 10271,   900,\n",
      "             4,     2,  -100,  -100,  -100,  -100],\n",
      "        [    0,   250,  1123, 23226,  5448,    61,  7544,    80,  6872, 33704,\n",
      "           994,   583,  1378, 10416,    94,    76,   197,    45,  1303, 20396,\n",
      "            50, 43337, 13014,   514,    53,  1492,  8182,    24,    40,   240,\n",
      "         12872,     6,  2320,   224,     4,     2],\n",
      "        [    0,  3048,  5919,   999, 18168,  1604,    34,    57,   963,    11,\n",
      "            10,   512,  3213,    14,   669,     7,     5,   744,     9,    10,\n",
      "          7004,    12,   180,    12,   279,   313,     4,     2,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100]]), 'inputs_embeds': tensor([[[ 0.5293, -0.2575, -0.0089,  ...,  0.6059,  0.8300, -0.3529],\n",
      "         [ 0.6181, -0.3857, -0.5376,  ...,  0.4051,  0.7915,  0.2080],\n",
      "         [ 0.1739, -0.9607, -0.2544,  ..., -0.3409,  0.0388, -0.0319],\n",
      "         ...,\n",
      "         [ 0.0191,  0.2744,  0.1521,  ..., -0.0435, -0.0552, -0.0577],\n",
      "         [ 0.0166, -0.0877, -0.0573,  ..., -0.1588, -0.1458, -0.0275],\n",
      "         [-0.0448,  0.4604, -0.0604,  ...,  0.1073,  0.0310,  0.0477]],\n",
      "\n",
      "        [[ 0.5293, -0.2575, -0.0089,  ...,  0.6059,  0.8300, -0.3529],\n",
      "         [ 0.6181, -0.3857, -0.5376,  ...,  0.4051,  0.7915,  0.2080],\n",
      "         [ 0.1739, -0.9607, -0.2544,  ..., -0.3409,  0.0388, -0.0319],\n",
      "         ...,\n",
      "         [-0.0395, -0.0538,  0.0469,  ...,  0.1042, -0.1165,  0.0928],\n",
      "         [-0.0522, -0.0330, -0.1346,  ...,  0.0259, -0.0416, -0.0339],\n",
      "         [-0.0448,  0.4604, -0.0604,  ...,  0.1073,  0.0310,  0.0477]],\n",
      "\n",
      "        [[ 0.5293, -0.2575, -0.0089,  ...,  0.6059,  0.8300, -0.3529],\n",
      "         [ 0.6181, -0.3857, -0.5376,  ...,  0.4051,  0.7915,  0.2080],\n",
      "         [ 0.1739, -0.9607, -0.2544,  ..., -0.3409,  0.0388, -0.0319],\n",
      "         ...,\n",
      "         [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
      "         [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
      "         [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087]],\n",
      "\n",
      "        [[ 0.5293, -0.2575, -0.0089,  ...,  0.6059,  0.8300, -0.3529],\n",
      "         [ 0.6181, -0.3857, -0.5376,  ...,  0.4051,  0.7915,  0.2080],\n",
      "         [ 0.1739, -0.9607, -0.2544,  ..., -0.3409,  0.0388, -0.0319],\n",
      "         ...,\n",
      "         [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
      "         [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
      "         [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087]]],\n",
      "       grad_fn=<CatBackward0>)}\n",
      "=> input_embeds : torch.Size([4, 1006, 1024])\n",
      "=> attention_mask : torch.Size([4, 1006])\n"
     ]
    }
   ],
   "source": [
    "inputs_embeds = torch.cat([prompt_embeds, input_embeds], dim=1)\n",
    "inputs['inputs_embeds'] = inputs_embeds\n",
    "\n",
    "prompt_length=200\n",
    "mask = torch.ones(batch_size, prompt_length, dtype=torch.long)\n",
    "inputs['attention_mask'] = torch.cat([mask, inputs['attention_mask']], dim=1)\n",
    "\n",
    "print(f'최종 inputs : {inputs}')\n",
    "print(f'=> input_embeds : {inputs[\"inputs_embeds\"].shape}')\n",
    "print(f'=> attention_mask : {inputs[\"attention_mask\"].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. model()\n",
    "일반적인 BART-large forward와 동일!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()\n",
    "del inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[    0,  4148,     5,    78,   183,    11,    39,    92,   633,     6,\n",
       "            732,  3540, 34597,  9430,    21,   576,    10,  5342,  2007,  4315,\n",
       "             35,    22,  6785,   213,   146,   201,    10,   319,     9,   418,\n",
       "             72,     2,  -100,  -100,  -100,  -100],\n",
       "         [    0,   133, 19716,   392,    16,   567,   223,  1164,     7,   224,\n",
       "            549,    79,  1467,    59,    10,   431,  3834,  7051,     9,     5,\n",
       "            987,    18,  1748,  2398,   467,   137,    10,  4096, 10271,   900,\n",
       "              4,     2,  -100,  -100,  -100,  -100],\n",
       "         [    0,   250,  1123, 23226,  5448,    61,  7544,    80,  6872, 33704,\n",
       "            994,   583,  1378, 10416,    94,    76,   197,    45,  1303, 20396,\n",
       "             50, 43337, 13014,   514,    53,  1492,  8182,    24,    40,   240,\n",
       "          12872,     6,  2320,   224,     4,     2],\n",
       "         [    0,  3048,  5919,   999, 18168,  1604,    34,    57,   963,    11,\n",
       "             10,   512,  3213,    14,   669,     7,     5,   744,     9,    10,\n",
       "           7004,    12,   180,    12,   279,   313,     4,     2,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100]]),\n",
       " 'inputs_embeds': tensor([[[ 0.5293, -0.2575, -0.0089,  ...,  0.6059,  0.8300, -0.3529],\n",
       "          [ 0.6181, -0.3857, -0.5376,  ...,  0.4051,  0.7915,  0.2080],\n",
       "          [ 0.1739, -0.9607, -0.2544,  ..., -0.3409,  0.0388, -0.0319],\n",
       "          ...,\n",
       "          [ 0.0191,  0.2744,  0.1521,  ..., -0.0435, -0.0552, -0.0577],\n",
       "          [ 0.0166, -0.0877, -0.0573,  ..., -0.1588, -0.1458, -0.0275],\n",
       "          [-0.0448,  0.4604, -0.0604,  ...,  0.1073,  0.0310,  0.0477]],\n",
       " \n",
       "         [[ 0.5293, -0.2575, -0.0089,  ...,  0.6059,  0.8300, -0.3529],\n",
       "          [ 0.6181, -0.3857, -0.5376,  ...,  0.4051,  0.7915,  0.2080],\n",
       "          [ 0.1739, -0.9607, -0.2544,  ..., -0.3409,  0.0388, -0.0319],\n",
       "          ...,\n",
       "          [-0.0395, -0.0538,  0.0469,  ...,  0.1042, -0.1165,  0.0928],\n",
       "          [-0.0522, -0.0330, -0.1346,  ...,  0.0259, -0.0416, -0.0339],\n",
       "          [-0.0448,  0.4604, -0.0604,  ...,  0.1073,  0.0310,  0.0477]],\n",
       " \n",
       "         [[ 0.5293, -0.2575, -0.0089,  ...,  0.6059,  0.8300, -0.3529],\n",
       "          [ 0.6181, -0.3857, -0.5376,  ...,  0.4051,  0.7915,  0.2080],\n",
       "          [ 0.1739, -0.9607, -0.2544,  ..., -0.3409,  0.0388, -0.0319],\n",
       "          ...,\n",
       "          [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
       "          [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
       "          [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087]],\n",
       " \n",
       "         [[ 0.5293, -0.2575, -0.0089,  ...,  0.6059,  0.8300, -0.3529],\n",
       "          [ 0.6181, -0.3857, -0.5376,  ...,  0.4051,  0.7915,  0.2080],\n",
       "          [ 0.1739, -0.9607, -0.2544,  ..., -0.3409,  0.0388, -0.0319],\n",
       "          ...,\n",
       "          [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
       "          [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
       "          [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087]]],\n",
       "        grad_fn=<CatBackward0>)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = self_model(**inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. Loss Function: CrossEntropyLoss\n",
    "\n",
    "단, 해당 loss는 Summarization의 경우에만 해당함. 다른 task에서는 각자 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 4.745816230773926\n"
     ]
    }
   ],
   "source": [
    "self_label_smoothing = 0.1\n",
    "loss_fct = nn.CrossEntropyLoss(label_smoothing=self_label_smoothing)\n",
    "vocab_size = outputs.logits.size(-1)\n",
    "\n",
    "logits = outputs.logits\n",
    "labels = inputs['labels']\n",
    "\n",
    "loss = loss_fct(logits.view(-1, vocab_size), labels.view(-1))\n",
    "print(f'loss : {loss}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextBox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
