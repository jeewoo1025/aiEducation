# All about Coding
`written by ì„¤ì§€ìš°` <br>
__Last Updated 2025-06-15__

## ğŸ“ Metrics

### 1. **Pass@k** (Functional Correctness)
kê°œì˜ ìƒ˜í”Œ ì¤‘ ìµœì†Œ í•˜ë‚˜ê°€ ì •ë‹µì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼í•˜ëŠ” ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë©”íŠ¸ë¦­ [[link](https://deepgram.com/learn/humaneval-llm-benchmark?utm_source=chatgpt.com)]

```python
# https://github.com/openai/human-eval/blob/master/human_eval/evaluation.py
def estimate_pass_at_k(
    num_samples: Union[int, List[int], np.ndarray],
    num_correct: Union[List[int], np.ndarray],
    k: int
) -> np.ndarray:
    """
    Estimates pass@k of each problem and returns them in an array.
    """

    def estimator(n: int, c: int, k: int) -> float:
        """
        Calculates 1 - comb(n - c, k) / comb(n, k).
        """
        if n - c < k:
            return 1.0
        return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))

    if isinstance(num_samples, int):
        num_samples_it = itertools.repeat(num_samples, len(num_correct))
    else:
        assert len(num_samples) == len(num_correct)
        num_samples_it = iter(num_samples)

    return np.array([estimator(int(n), int(c), k) for n, c in zip(num_samples_it, num_correct)])
```

### 2. **Resolved Rate** (Issue Resolution)
ì‹¤ì œ ì†Œí”„íŠ¸ì›¨ì–´ ì´ìŠˆë¥¼ ì„±ê³µì ìœ¼ë¡œ í•´ê²°í•œ ë¹„ìœ¨ì„ ì¸¡ì •í•˜ëŠ” ë©”íŠ¸ë¦­ (SWE-benchì—ì„œ ì‚¬ìš©)

```python
# SWE-bench evaluation process
def calculate_resolved_rate(instances_resolved: int, total_instances: int) -> float:
    """
    Calculate the percentage of instances successfully resolved.
    
    Args:
        instances_resolved: Number of instances where patch fixed the issue
        total_instances: Total number of instances in the dataset
    
    Returns:
        Resolved rate as percentage
    """
    return (instances_resolved / total_instances) * 100.0

# Evaluation steps:
# 1. Set up Docker environment for repository
# 2. Apply model's generated patch
# 3. Run repository's test suite
# 4. Determine if patch successfully resolves the issue
```

### 3. **BLEU** (Syntactic Similarity)
ìƒì„±ëœ ì½”ë“œê°€ ì •ë‹µ ì½”ë“œì™€ í† í° ìˆ˜ì¤€ì—ì„œ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ í‰ê°€

```python
# https://github.com/hendrycks/apps/blob/main/eval/eval_bleu.py

import sacrebleu
from sacremoses import MosesDetokenizer
md = MosesDetokenizer(lang='en')

random.seed(12345678987654321)

def calc_bleu(output: List[str], targets: List[List[str]]):
    """
    Calculate BLEU score for generated code against reference solutions.
    """
    max_bleu = 0
    bleu = sacrebleu.corpus_bleu(output, targets)
    for item in targets[0]:
        tmp_bleu = sacrebleu.corpus_bleu(output, [[item]])
        if tmp_bleu.score > max_bleu:
            max_bleu = tmp_bleu.score
    return bleu.score, max_bleu
```

### 4. **CodeBLEU** (Syntactic + Semantic Similarity)
BLEUì˜ í™•ì¥ìœ¼ë¡œ AST(Abstract Syntax Tree)ì™€ ë°ì´í„° í”Œë¡œìš°ë¥¼ ê³ ë ¤í•œ ì½”ë“œ íŠ¹í™” ë©”íŠ¸ë¦­

```python
# CodeBLEU combines multiple components:
# - BLEU: n-gram overlap
# - Weighted n-gram match: syntax-aware token matching
# - AST match: syntactic structure similarity  
# - Data-flow match: semantic variable relationships

def calculate_codebleu(
    predictions: List[str], 
    references: List[List[str]], 
    lang: str,
    weights: Tuple[float, float, float, float] = (0.25, 0.25, 0.25, 0.25)
) -> float:
    """
    Calculate CodeBLEU score.
    
    Args:
        predictions: Generated code snippets
        references: Reference code solutions
        lang: Programming language
        weights: Weights for (bleu, weighted_ngram, ast, dataflow)
    
    Returns:
        CodeBLEU score (0-100)
    """
    # Implementation involves AST parsing and dataflow analysis
    pass
```

### 5. **Execution Accuracy** (Binary Correctness)
ìƒì„±ëœ ì½”ë“œê°€ ì£¼ì–´ì§„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ëª¨ë‘ í†µê³¼í•˜ëŠ”ì§€ ì´ì§„ í‰ê°€

```python
def execution_accuracy(generated_code: str, test_cases: List[dict]) -> bool:
    """
    Evaluate if generated code passes all test cases.
    
    Args:
        generated_code: The generated code snippet
        test_cases: List of input-output test cases
    
    Returns:
        True if all test cases pass, False otherwise
    """
    try:
        exec(generated_code, globals())
        for test_case in test_cases:
            result = eval(f"solution({test_case['input']})")
            if result != test_case['expected_output']:
                return False
        return True
    except Exception:
        return False
```

### 6. **Edit Similarity** (Code Modification)
ì›ë³¸ ì½”ë“œ ëŒ€ë¹„ ìˆ˜ì •ëœ ë¶€ë¶„ì˜ ì •í™•ì„±ì„ í‰ê°€ (code editing ì‘ì—…ìš©)

```python
import difflib

def edit_similarity(original: str, generated: str, reference: str) -> float:
    """
    Calculate edit similarity based on diff operations.
    
    Args:
        original: Original buggy code
        generated: Model-generated fixed code  
        reference: Reference solution
    
    Returns:
        Similarity score (0-1)
    """
    gen_diff = list(difflib.unified_diff(original.split('\n'), generated.split('\n')))
    ref_diff = list(difflib.unified_diff(original.split('\n'), reference.split('\n')))
    
    # Compare edit operations
    common_edits = set(gen_diff) & set(ref_diff)
    total_edits = len(set(gen_diff) | set(ref_diff))
    
    return len(common_edits) / total_edits if total_edits > 0 else 0.0
```

## ë©”íŠ¸ë¦­ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

| ë©”íŠ¸ë¦­ | í‰ê°€ ëŒ€ìƒ | ì¥ì  | ë‹¨ì  | ì ìš© ë²¤ì¹˜ë§ˆí¬ |
|--------|-----------|------|------|---------------|
| **Pass@k** | ê¸°ëŠ¥ì  ì •í™•ì„± | â€¢ ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ ê¸°ë°˜<br>â€¢ ë‹¤ì–‘í•œ ì†”ë£¨ì…˜ í—ˆìš© | â€¢ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í’ˆì§ˆ ì˜ì¡´<br>â€¢ ê³„ì‚° ë¹„ìš© ë†’ìŒ | HumanEval, MBPP, LiveCodeBench |
| **Resolved Rate** | ì‹¤ë¬´ ë¬¸ì œ í•´ê²° | â€¢ í˜„ì‹¤ì  í‰ê°€<br>â€¢ ì „ì²´ ì›Œí¬í”Œë¡œìš° ê³ ë ¤ | â€¢ í™˜ê²½ ì„¤ì • ë³µì¡<br>â€¢ ì¬í˜„ì„± ì´ìŠˆ | SWE-bench, SWE-bench Verified |
| **BLEU** | êµ¬ë¬¸ì  ìœ ì‚¬ì„± | â€¢ ë¹ ë¥¸ ê³„ì‚°<br>â€¢ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ê²€ì¦ë¨ | â€¢ ì½”ë“œ íŠ¹ì„± ë¯¸ë°˜ì˜<br>â€¢ ì •í™•í•œ ì½”ë“œë„ ë‚®ì€ ì ìˆ˜ | APPS (ë³´ì¡° ë©”íŠ¸ë¦­) |
| **CodeBLEU** | êµ¬ë¬¸+ì˜ë¯¸ì  ìœ ì‚¬ì„± | â€¢ ì½”ë“œ êµ¬ì¡° ê³ ë ¤<br>â€¢ ASTì™€ ë°ì´í„°í”Œë¡œìš° ë°˜ì˜ | â€¢ ì–¸ì–´ë³„ íŒŒì„œ í•„ìš”<br>â€¢ ë³µì¡í•œ ê³„ì‚° | CodeXGLUE, ì½”ë“œ ë²ˆì—­ ì‘ì—… |
| **Execution Accuracy** | ì´ì§„ ì •í™•ì„± | â€¢ ëª…í™•í•œ ê¸°ì¤€<br>â€¢ ë¹ ë¥¸ í‰ê°€ | â€¢ ë¶€ë¶„ ì ìˆ˜ ì—†ìŒ<br>â€¢ ì—„ê²©í•œ í‰ê°€ | ì´ˆê¸° ì½”ë“œ ìƒì„± ì—°êµ¬ |
| **Edit Similarity** | ìˆ˜ì • ì •í™•ì„± | â€¢ ìˆ˜ì • ì‘ì—…ì— íŠ¹í™”<br>â€¢ ë³€ê²½ëŸ‰ ê³ ë ¤ | â€¢ êµ¬í˜„ ë³µì¡<br>â€¢ ê¸°ì¤€ ëª¨í˜¸ | ì½”ë“œ ìˆ˜ì •, ë²„ê·¸ í”½ìŠ¤ |

## ê¶Œì¥ ì‚¬ìš©ë²•

1. **ê¸°ë³¸ í‰ê°€**: Pass@1, Pass@10ì„ ì£¼ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‚¬ìš©
2. **ë³´ì¡° í‰ê°€**: CodeBLEUë¡œ êµ¬ì¡°ì  ìœ ì‚¬ì„± ì¸¡ì •  
3. **ì‹¤ë¬´ í‰ê°€**: SWE-benchì˜ Resolved Rateë¡œ ì‹¤ì œ ì ìš©ì„± ê²€ì¦
4. **ë‹¤ì¤‘ ë©”íŠ¸ë¦­**: ì—¬ëŸ¬ ê´€ì ì—ì„œ ì¢…í•©ì  í‰ê°€ ê¶Œì¥

<br>

## ğŸ“Š Benchmark Overview

| ë²¤ì¹˜ë§ˆí¬ ì´ë¦„ | ì§€ì› ì–¸ì–´ | ì–¸ì–´ ìˆ˜ | ë¬¸ì œ ìˆ˜ | ì£¼ìš” ë©”íŠ¸ë¦­ | ì„¤ëª… | ë§í¬ |
|-------------|----------|---------|---------|------------|------|------|
| **HumanEval** | Python | 1ê°œ | 164ê°œ | Pass@1, Pass@10, Pass@100 | OpenAIì—ì„œ ê°œë°œí•œ í•¨ìˆ˜ ìˆ˜ì¤€ì˜ ì½”ë“œ ìƒì„± ë²¤ì¹˜ë§ˆí¬. ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê¸°ì¤€ì  | [GitHub](https://github.com/openai/human-eval) |
| **HumanEval+** | Python | 1ê°œ | 164ê°œ | Pass@1, Pass@10, Pass@100 | HumanEvalì˜ ê°œì„  ë²„ì „ìœ¼ë¡œ ë” ì—„ê²©í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í¬í•¨ | [GitHub](https://github.com/evalplus/evalplus) |
| **HumanEval-X** | Python, C++, Java, JavaScript, Go | 5ê°œ | 164ê°œ Ã— 5ê°œ ì–¸ì–´ | Pass@1, Pass@10, Pass@100 | HumanEvalì„ ë‹¤ì¤‘ ì–¸ì–´ë¡œ í™•ì¥í•œ ë²¤ì¹˜ë§ˆí¬ | [GitHub](https://github.com/THUDM/CodeGeeX) |
| **MBPP** | Python | 1ê°œ | 974ê°œ | Pass@1, Pass@10, Pass@100 | Mostly Basic Programming Problems. ì´ˆê¸‰~ì¤‘ê¸‰ ìˆ˜ì¤€ í”„ë¡œê·¸ë˜ë° ë¬¸ì œ | [GitHub](https://github.com/google-research/google-research/tree/master/mbpp) |
| **MBPP+** | Python | 1ê°œ | 974ê°œ | Pass@1, Pass@10, Pass@100 | MBPPì˜ ê°œì„  ë²„ì „ìœ¼ë¡œ ë” ì—„ê²©í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í¬í•¨ | [GitHub](https://github.com/evalplus/evalplus) |
| **CodeContests** | C++, Python, Java | 3ê°œ | 13,610ê°œ | Pass@1, Pass@10, Pass@100 | DeepMindì˜ ê²½ìŸ í”„ë¡œê·¸ë˜ë° ë¬¸ì œ ë°ì´í„°ì…‹ | [GitHub](https://github.com/deepmind/code_contests) |
| **MultiPL-E** | Python, C++, Java, JavaScript, TypeScript, PHP, C#, Bash, R, Perl, Scala, Swift, Rust, Go, D, Julia, Lua, Racket | 18ê°œ | 164ê°œ Ã— 18ê°œ ì–¸ì–´ | Pass@1, Pass@10, Pass@100 | HumanEvalì„ 18ê°œ ì–¸ì–´ë¡œ í™•ì¥í•œ ëŒ€ê·œëª¨ ë‹¤êµ­ì–´ ë²¤ì¹˜ë§ˆí¬ | [GitHub](https://github.com/nuprl/MultiPL-E) |
| **LiveCodeBench** | Python, C++, Java, JavaScript | 4ê°œ | 300+ê°œ (ì§€ì† ì¦ê°€) | Pass@1 | ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ëŠ” ì˜¤ì—¼ ë°©ì§€ ë²¤ì¹˜ë§ˆí¬. LeetCode, AtCoder, CodeForcesì—ì„œ ìˆ˜ì§‘ | [Website](https://livecodebench.github.io/) |
| **BigCodeBench** | Python | 1ê°œ | 1,140ê°œ | Pass@1, Pass@10, Pass@100 | HumanEvalì˜ ì°¨ì„¸ëŒ€ ë²¤ì¹˜ë§ˆí¬ë¡œ ë” ë³µì¡í•˜ê³  ì‹¤ìš©ì ì¸ ì½”ë”© ì‘ì—… í¬í•¨ | [GitHub](https://github.com/bigcode-project/bigcodebench) |
| **DS-1000** | Python | 1ê°œ | 1,000ê°œ | Pass@1, Pass@10, Pass@100 | ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì‘ì—…ì— íŠ¹í™”ëœ ë²¤ì¹˜ë§ˆí¬ (NumPy, Pandas, SciPy, Matplotlib ë“±) | [GitHub](https://github.com/HKUNLP/DS-1000) |
| **APPS** | Python | 1ê°œ | 10,000ê°œ | Pass@1, Pass@5, Pass@100 | ê²½ìŸ í”„ë¡œê·¸ë˜ë°ê³¼ ë©´ì ‘ ë¬¸ì œë¥¼ í¬í•¨í•œ ëŒ€ê·œëª¨ ë²¤ì¹˜ë§ˆí¬ | [GitHub](https://github.com/hendrycks/apps) |
| **SWE-bench** | Python | 1ê°œ | 2,294ê°œ | Pass@1, Resolved Rate | ì‹¤ì œ GitHub ì €ì¥ì†Œì˜ ì´ìŠˆ í•´ê²° ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§ ë²¤ì¹˜ë§ˆí¬ | [GitHub](https://github.com/princeton-nlp/SWE-bench) |

## ì£¼ìš” íŠ¹ì§•

### ì–¸ì–´ ì§€ì› í˜„í™©
- **ë‹¨ì¼ ì–¸ì–´ (Pythonë§Œ)**: HumanEval, MBPP, BigCodeBench, DS-1000, APPS, SWE-bench
- **ì†Œìˆ˜ ì–¸ì–´ (2-5ê°œ)**: HumanEval-X (5ê°œ), CodeContests (3ê°œ), LiveCodeBench (4ê°œ)
- **ë‹¤êµ­ì–´ (10ê°œ ì´ìƒ)**: MultiPL-E (18ê°œ)

### ë©”íŠ¸ë¦­ ì„¤ëª…
- **Pass@1**: 1ë²ˆ ì‹œë„ì—ì„œ ì •ë‹µì„ ìƒì„±í•  í™•ë¥ 
- **Pass@10**: 10ë²ˆ ì‹œë„ ì¤‘ ì ì–´ë„ 1ë²ˆ ì •ë‹µì„ ìƒì„±í•  í™•ë¥   
- **Pass@100**: 100ë²ˆ ì‹œë„ ì¤‘ ì ì–´ë„ 1ë²ˆ ì •ë‹µì„ ìƒì„±í•  í™•ë¥ 
- **Resolved Rate**: ì‹¤ì œ ì†Œí”„íŠ¸ì›¨ì–´ ì´ìŠˆ í•´ê²° ì„±ê³µë¥ 

### ë²¤ì¹˜ë§ˆí¬ íŠ¸ë Œë“œ
1. **í¬í™” í˜„ìƒ**: HumanEval Pass@1ì´ 99.4%, MBPPê°€ 94.2%ì— ë„ë‹¬í•˜ì—¬ ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ í•„ìš”ì„± ì¦ëŒ€
2. **ì˜¤ì—¼ ë°©ì§€**: LiveCodeBenchëŠ” ì‹œê°„ë³„ ë¬¸ì œ ìˆ˜ì§‘ìœ¼ë¡œ ë°ì´í„° ì˜¤ì—¼ ë°©ì§€ (2025.06 ê¸°ì¤€ `release_v6` ê³µê°œ)
3. **ì‹¤ìš©ì„± ê°•í™”**: BigCodeBench, SWE-bench ë“± ì‹¤ì œ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ì‹œë‚˜ë¦¬ì˜¤ ë°˜ì˜
4. **ë‹¤êµ­ì–´ í™•ì¥**: MultiPL-EëŠ” 18ê°œ ì–¸ì–´ ì§€ì›ìœ¼ë¡œ ê°€ì¥ í¬ê´„ì 

<br>

## Github Example Code
### BigCode Evaluation Harness 
- GitHub: https://github.com/bigcode-project/bigcode-evaluation-harness
- ì£¼ìš” íŠ¹ì§•:
    - EleutherAI/lm-evaluation-harnessì—ì„œ ì˜ê°ì„ ë°›ì€ ì½”ë“œ ìƒì„± ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬
    - ë°°ì¹˜ ì¶”ë¡  ì§€ì› (--batch_size ì˜µì…˜)
    - HumanEval, MBPP, MultiPL-E, DS-1000 ë“± ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ì§€ì›
    - ë©€í‹° GPU ì§€ì› (accelerate launch)

```bash
accelerate launch main.py \
    --model codellama/CodeLlama-7b-Python-hf \
    --tasks humaneval \
    --temperature 0.1 \
    --n_samples 10 \
    --batch_size 8 \
    --allow_code_execution
```

### Basic Code: Eval StarCoder
- [abacaj/code-eval](https://github.com/abacaj/code-eval/tree/main)
```python
from transformers import (
    AutoTokenizer,
    GPTBigCodeForCausalLM,
    PreTrainedTokenizer,
    PreTrainedModel,
)
from core import run_eval, filter_code, fix_indents
import os
import torch

# TODO: move to python-dotenv
# add hugging face access token here
TOKEN = ""


@torch.inference_mode()
def generate_batch_completion(
    model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prompt: str, batch_size: int
) -> list[str]:
    input_batch = [prompt for _ in range(batch_size)]
    inputs = tokenizer(input_batch, return_tensors="pt").to(model.device)
    input_ids_cutoff = inputs.input_ids.size(dim=1)

    generated_ids = model.generate(
        **inputs,
        use_cache=True,
        max_new_tokens=512,
        temperature=0.2,
        top_p=0.95,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,  # model has no pad token
    )

    batch_completions = tokenizer.batch_decode(
        [ids[input_ids_cutoff:] for ids in generated_ids],
        skip_special_tokens=True,
    )

    # fix_indents is required to fix the tab character that is generated from starcoder model
    return [filter_code(fix_indents(completion)) for completion in batch_completions]


if __name__ == "__main__":
    # adjust for n = 10 etc
    num_samples_per_task = 10
    out_path = "results/starcoder/eval.jsonl"
    os.makedirs("results/starcoder", exist_ok=True)

    tokenizer = AutoTokenizer.from_pretrained(
        "bigcode/starcoder",
        trust_remote_code=True,
        use_auth_token=TOKEN,
    )

    model = torch.compile(
        GPTBigCodeForCausalLM.from_pretrained(
            "bigcode/starcoder",
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            max_memory={
                0: "18GiB",
                1: "18GiB",
            },
            use_auth_token=TOKEN,
        ).eval()
    )

    run_eval(
        model,
        tokenizer,
        num_samples_per_task,
        out_path,
        generate_batch_completion,
        True,
    )
```

<br>

## ğŸ“š References

- **HumanEval**  
  Chen et al., *Evaluating Large Language Models Trained on Code*, arXiv 2021.  
  [[Paper](https://arxiv.org/abs/2107.03374)] â€¢ [[GitHub](https://github.com/openai/human-eval)]

- **MBPP (Mostly Basic Programming Problems)**  
  Austin et al., *Program Synthesis with Large Language Models*, arXiv 2021.  
  [[Paper](https://arxiv.org/abs/2108.07732)] â€¢ [[GitHub](https://github.com/google-research/google-research/tree/master/mbpp)]

- **APPS**  
  Hendrycks et al., *Measuring Coding Challenge Competence With APPS*, NeurIPS 2021.  
  [[Paper](https://arxiv.org/abs/2105.09938)] â€¢ [[GitHub](https://github.com/hendrycks/apps)]

- **CodeContests**  
  Nijkamp et al., *CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis*, arXiv 2022.  
  [[Paper](https://arxiv.org/abs/2203.13474)] â€¢ [[GitHub](https://github.com/salesforce/CodeGen)]

- **DS-1000**  
  Hong et al., *A Data Science Benchmark for Code Generation: DS-1000*, EMNLP 2023.  
  [[Paper](https://arxiv.org/abs/2305.14764)] â€¢ [[GitHub](https://github.com/HKUNLP/DS-1000)]

- **SWE-bench**  
  Khatami et al., *SWE-bench: Can Language Models Resolve Real-World GitHub Issues?*, arXiv 2023.  
  [[Paper](https://arxiv.org/abs/2310.06770)] â€¢ [[Website](https://www.swebench.com/)] â€¢ [[GitHub](https://github.com/princeton-nlp/SWE-bench)]

- **SWE-bench Lite / Multimodal / Verified**  
  SWE-bench íŒ€ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ê¸°ë°˜.  
  [[Lite](https://www.swebench.com/lite.html)] â€¢ [[Multimodal](https://www.swebench.com/multimodal.html)]

- **LeetCode**  
  https://leetcode.com (ë¬¸ì œ ë° í…ŒìŠ¤íŠ¸ ê¸°ë°˜ ë²¤ì¹˜ë§ˆí‚¹, ê³µì‹ ë…¼ë¬¸ ì—†ìŒ)

---
